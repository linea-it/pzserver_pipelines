#!/usr/bin/env python3

import argparse
import datetime
import os
import time
from pathlib import Path

import lsdb
from dask.distributed import Client
from executor import get_executor
from product_handle import ProductHandle, create_output
from utils import (
    create_logdir,
    dump_yml,
    load_yml,
    setup_logger,
    copy_files_by_extensions,
    copy_file,
    copy_directory,
)


class Runner:
    """Runner for TSM pipeline
    This class handles the execution of the TSM pipeline, including loading configurations,
    managing inputs and outputs, and executing the cross-matching process.
    """

    def __init__(self, config, cwd=".") -> None:
        """Initialize the Runner with configuration and working directory.

        Args:
            config (str): Path to the configuration file.
            cwd (str, optional): Working directory. Defaults to ".".

        Configuration file should contain:
            - executor: Executor configuration for Dask.
            - output_name: Name of the output file.
            - output_format: Format of the output file (e.g., parquet).
            - output_root_dir: Root directory for outputs.
            - output_dir: Directory for outputs relative to output_root_dir.
            - inputs: Dictionary containing input data specifications.
            - param: Dictionary containing parameters for the pipeline.
        """
        # Create logs directory
        self.logdir = create_logdir(cwd)

        # Setup logger
        self.logger = setup_logger("tsm", logdir=self.logdir)

        # Set up working directory
        self.cwd = cwd

        # Set up process info
        self.process_info_path = Path(self.cwd, "process.yml")

        # Creating process info yaml
        if not self.process_info_path.is_file():
            self.process_info_path.touch()

        # Loading process info yaml
        process_info = load_yml(self.process_info_path)
        if not process_info:
            process_info = {}
        self.process_info = process_info

        # Set up initial time tracking
        self.add_info("start_time", datetime.datetime.now())
        self.start_time = time.time()
        self.end_time = None

        # Loading config
        pipe_config = load_yml(config)

        self.logger.debug("Configurations: %s", pipe_config)

        # Retrieves the output directory to copy the output to at the end of processing.
        self.output_root_dir = pipe_config.get("output_root_dir", ".")
        self.output_dir = pipe_config.get("output_dir", "out")
        self.output_name = pipe_config.get("output_name", "tsm")
        self.output_format = pipe_config.get("output_format", None)

        if not Path(self.output_root_dir).is_absolute():
            self.output_root_dir = str(Path(self.cwd, self.output_root_dir))

        # Loading inputs
        self.inputs = pipe_config.get("inputs")
        self.logger.info("Inputs: %s", self.inputs)

        # Loading parameters
        self.param = pipe_config.get("param", {})
        self.param["crossmatch"] = self.param.get("crossmatch", {})

        self.specz_suffix = "_specz"
        self.param["crossmatch"]["suffixes"] = [self.specz_suffix, ""]

        self.logger.info("Params: %s", self.param)

        # Set executors
        self.executors = pipe_config.get("executor")

        # Create output dir
        output_full_dir = Path(self.output_root_dir, self.output_dir)
        os.makedirs(output_full_dir, exist_ok=True)

        # Get dask executor
        self.cluster = get_executor(self.executors)

    def __enter__(self):
        return self

    def add_info(self, key, value):
        """Add info in process.yaml"""

        self.process_info[key] = value  # type: ignore
        dump_yml(self.process_info_path, self.process_info)

    def run(self):
        """Run TSM"""

        specz = self.inputs.get("specz")
        if len(specz) > 1:
            self.logger.warn(f"more than one specz was selected: {specz}")

        specz = specz.pop()
        self.logger.debug(f"Specz used: {specz}")

        with Client(self.cluster) as client:
            df = ProductHandle().df_from_file(specz.get("path"))

            columns = specz.get("columns", {})

            ra = columns.get("ra", "ra")
            dec = columns.get("dec", "dec")

            specz_df = lsdb.from_dataframe(df, use_pyarrow_types=False, ra_column=ra, dec_column=dec)

            # dataset_path = self.inputs.get("dataset").get("path")
            # margin_cache_path = self.inputs.get("dataset").get("margin_path", None)
            dataset_path, margin_cache_path = self.__get_photometric_data_path()
            dataset_id = self.inputs.get("dataset").get("columns", {}).get("id", "id")

            if not os.path.exists(dataset_path):
                self.logger.error("Dataset path does not exist: %s", str(dataset_path))
                raise FileNotFoundError(
                    f"Dataset path does not exist: {str(dataset_path)}"
                )

            dataset = lsdb.read_hats(dataset_path, margin_cache=margin_cache_path)
            duplicate_criteria = self.param.get("duplicate_criteria", "closest")
            cm_args = self.param.get("crossmatch", {})

            try:
                cross = specz_df.crossmatch(dataset, **cm_args)
                if duplicate_criteria == "closest":
                    suffix_dataset = cm_args.get("suffixes")[1]
                    _id = f"{dataset_id}{suffix_dataset}"
                    cross = cross._ddf.shuffle(on=_id).map_partitions(
                        lambda x: x.sort_values("_dist_arcsec").drop_duplicates(_id)
                    )
                data = cross.compute()
                client.close()
            except ValueError as err:
                if "size 0 inputs" in str(err):
                    self.logger.exception("No cross matches were found!")
                    raise
                self.logger.exception("Error when cross-matching")
                raise

            suffix = self.specz_suffix
            data_columns = data.columns
            specz_columns = {}

            self.logger.debug(f"Output columns: {data_columns}")

            for col in data_columns:
                if col.endswith(suffix):
                    specz_columns[col] = col.removesuffix(suffix)

            for key, value in specz_columns.copy().items():
                if value in data_columns:
                    del specz_columns[key]

            if specz_columns:
                self.logger.debug(f"Renaming columns: {specz_columns}")
                data.rename(columns=specz_columns, inplace=True)

            if not self.output_format:
                self.output_format = specz.get("format")

            outputfile = create_output(
                data,
                self.output_root_dir,
                self.output_dir,
                self.output_name,
                self.output_format,
            )

            columns_suffix = {k: f"{v}{self.specz_suffix}" for k, v in columns.items()}
            columns_assoc = self.__get_columns_associations(
                columns_suffix, specz_columns
            )

            self.__register_outputs(self.output_root_dir, outputfile, columns_assoc)

            self.logger.debug("--> Object Count: \n%s", str(data.count()))

        self.end_time = time.time() - self.start_time
        self.logger.info("Time elapsed: %s", str(self.end_time))

        copy_files_by_extensions(".", self.logdir, [".log", ".err", ".out", ".yml"])
        copy_directory(
            self.logdir,
            str(Path(self.output_root_dir, self.output_dir, "process_info")),
        )

    def __get_photometric_data_path(self):
        """Get photometric data path and margin cache path

        Returns:
            tuple: dataset path and margin cache path
        """
        dataset_path = self.inputs.get("dataset").get("path")

        flux_type = self.param.get("flux_type")
        convert_flux_to_mag = self.param.get("convert_flux_to_mag")

        if convert_flux_to_mag:
            flux_or_mag = "mag"
        else:
            flux_or_mag = "flux"

        dereddening = self.param.get("dereddening")

        ptmd_path = Path(dataset_path, flux_or_mag, flux_type, dereddening, "catalog")

        margin_cache_path = Path(
            dataset_path, flux_or_mag, flux_type, dereddening, "catalog_1arcsec"
        )

        if not margin_cache_path.exists():
            margin_cache_path = None

        return str(ptmd_path), margin_cache_path

    def __get_columns_associations(self, columns, specz_columns):
        """Get columns associations of the product output

        Args:
            columns (dict): used columns
            specz_columns (dict): output columns
        """

        return {
            k: specz_columns[v] if v in specz_columns else v for k, v in columns.items()
        }

    def __register_outputs(self, root_dir, filepath, assoc, role="main"):
        """Register outputs in process.yml

        Args:
            root_dir (str): output root dir
            filepath (str): output path
            assoc (dic): columns association
            role (str, optional): role name. Defaults to 'main'.
        """

        outpath = str(Path(filepath))
        outputs = self.process_info.get("outputs", [])  # type: ignore
        outputs.append(
            {
                "path": outpath,
                "root_dir": root_dir,
                "role": role,
                "columns_assoc": assoc,
            }
        )
        self.add_info("outputs", outputs)

    def __add_train_flag(self):
        """Add train/test flag"""
        raise NotImplementedError

    def __exit__(self, exc_type, exc_value, traceback):

        self.cluster.close()
        self.add_info("end_time", datetime.datetime.now())

        if exc_type:
            self.logger.error("%s: %s", exc_type.__name__, exc_value)
            self.logger.debug("Traceback: %s", traceback)
            self.add_info("status", "Failed")
        else:
            self.add_info("status", "Successful")

        copy_file(
            self.process_info_path,
            str(Path(self.output_root_dir, self.output_dir, "process.yml")),
        )


if __name__ == "__main__":
    # Create the parser and add arguments
    parser = argparse.ArgumentParser()
    parser.add_argument(dest="config_path", help="yaml config path")
    parser.add_argument(
        dest="cwd", nargs="?", help="processing dir", default=os.getcwd()
    )

    args = parser.parse_args()
    config_path = args.config_path

    # Run pipeline
    with Runner(config_path, args.cwd) as tsmrun:
        tsmrun.run()
