#!/usr/bin/env python3

import argparse
import datetime
import os
import time
from pathlib import Path
import tables_io

import lsdb
from dask.distributed import Client
from executor import get_executor
from utils import dump_yml, load_yml, setup_logger


class Runner:

    def __init__(self, config, cwd=".") -> None:
        self.logger = setup_logger("tsm", logdir=cwd)
        self.cwd = cwd
        self.process_info_path = Path(self.cwd, "process.yml")
        self.process_info = None
        self.start_time = None
        self.end_time = None

        # Loading config
        pipe_config = load_yml(config)

        # Define output dir
        self.output_dir = Path(pipe_config.get("output_dir", "out"))
        if not self.output_dir.is_absolute():
            self.output_dir = Path(self.cwd, self.output_dir)

        # Loading inputs
        self.inputs = pipe_config.get("inputs")
        self.logger.info("Inputs: %s", self.inputs)

        # Loading parameters
        self.param = pipe_config.get("param", {})
        self.param["crossmatch"] = self.param.get("crossmatch", {})

        if not self.param.get("crossmatch").get("suffixes", None):
            self.param["crossmatch"]["suffixes"] = ["_specz", ""]

        self.logger.info("Params: %s", self.param)

        # Set executors
        self.executors = pipe_config.get("executor")

        # Create output dir
        os.makedirs(self.output_dir, exist_ok=True)

        # Creating process info yaml
        if not self.process_info_path.is_file():
            self.process_info_path.touch()

        # Loading process info yaml
        process_info = load_yml(self.process_info_path)
        if not process_info:
            process_info = {}
        self.process_info = process_info

        # Get dask executor
        executor_key = os.getenv("DASK_EXECUTOR_KEY", "slurm")
        self.cluster = get_executor(executor_key, self.executors)

    def __enter__(self):
        return self

    def add_info(self, key, value):
        """Add info in process.yaml"""

        self.process_info[key] = value
        dump_yml(self.process_info_path, self.process_info)

    def run(self):
        """Run TSM"""

        # Adding start time
        self.start_time = time.time()
        self.add_info("start_time", datetime.datetime.now())

        specz = self.inputs.get("specz")
        if len(specz) > 1:
            self.logger.warn(f"more than one specz was selected: {specz}")

        specz = specz.pop()
        self.logger.info(f"Specz used: {specz}")

        with Client(self.cluster) as client:
            specz_df = lsdb.from_dataframe(
                tables_io.read(specz.get("path"), tables_io.types.PD_DATAFRAME)
            )

            dataset_path = self.inputs.get("dataset").get("path")
            dataset_id = self.inputs.get("dataset").get("columns", {}).get("id", {})

            dataset = lsdb.read_hipscat(dataset_path)
            duplicate_criteria = self.param.get("duplicate_criteria", "closest")
            cm_args = self.param.get("crossmatch", {})

            try:
                cross = specz_df.crossmatch(dataset, **cm_args)
                if duplicate_criteria == "closest":
                    suffix_dataset = cm_args.get("suffixes")[1]
                    _id = f"{dataset_id}{suffix_dataset}"
                    cross = cross._ddf.shuffle(on=_id).map_partitions(
                        lambda x: x.sort_values("_dist_arcsec").drop_duplicates(_id)
                    )
                data = cross.compute()
            except ValueError as err:
                if "size 0 inputs" in str(err):
                    self.logger.exception("No cross matches were found!")
                    raise
                self.logger.exception("Error when cross-matching")
                raise

            outputfile = Path(self.output_dir, "tsm-output.parquet")
            data.to_parquet(outputfile)
            self.__register_outputs(outputfile)

            self.logger.info("--> Object Count: \n%s", str(data.count()))

        self.end_time = time.time() - self.start_time
        self.logger.info("Time elapsed: %s", str(self.end_time))

    def __register_outputs(self, filepath, role="main"):
        """Register outputs in process.yml

        Args:
            filepath (str): output path
            role (str, optional): role name. Defaults to 'main'.
        """

        outpath = str(Path(filepath).resolve())
        outputs = self.process_info.get("outputs", [])
        outputs.append({"path": outpath, "role": role})
        self.add_info("outputs", outputs)

    def __add_train_flag(self):
        """Add train/test flag"""
        raise NotImplementedError

    def __exit__(self, exc_type, exc_value, traceback):

        self.cluster.close()
        self.add_info("end_time", datetime.datetime.now())

        if exc_type:
            self.logger.error("%s: %s", exc_type.__name__, exc_value)
            self.logger.debug("Traceback: %s", traceback)
            self.add_info("status", "Failed")
        else:
            self.add_info("status", "Successful")


if __name__ == "__main__":
    # Create the parser and add arguments
    parser = argparse.ArgumentParser()
    parser.add_argument(dest="config_path", help="yaml config path")
    parser.add_argument(
        dest="cwd", nargs="?", help="processing dir", default=os.getcwd()
    )

    args = parser.parse_args()
    config_path = args.config_path
    cwd = args.cwd

    # Run pipeline
    with Runner(config_path, cwd) as tsmrun:
        tsmrun.run()
