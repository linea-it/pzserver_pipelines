{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c8de9-a3dd-4059-82e0-555daa169f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import warnings\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "import hats\n",
    "from hats_import.pipeline import ImportArguments, pipeline_with_client\n",
    "from hats_import.catalog.file_readers import ParquetReader\n",
    "from hats_import.margin_cache.margin_cache_arguments import MarginCacheArguments\n",
    "import lsdb\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------\n",
    "# INITIAL CONFIGURATION\n",
    "# ----------------------------\n",
    "path_to_yaml_file = \"/scratch/users/luigi.silva/pzserver_pipelines/combine_specz/notebooks/config.yaml\"\n",
    "\n",
    "with open(path_to_yaml_file) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "base_dir = config[\"base_dir\"]\n",
    "output_dir = os.path.join(base_dir, config[\"output_dir\"])\n",
    "logs_dir = os.path.join(base_dir, config[\"logs_dir\"])\n",
    "temp_dir = os.path.join(base_dir, config[\"temp_dir\"])\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "catalogs = config[\"inputs\"][\"specz\"]\n",
    "\n",
    "# ----------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# ----------------------------\n",
    "\n",
    "def setup_cluster(config, logs_dir):\n",
    "    executor = config[\"executor\"]\n",
    "    if executor[\"name\"] == \"slurm\":\n",
    "        instance = executor[\"args\"][\"instance\"]\n",
    "        adapt = executor[\"args\"].get(\"adapt\", {})\n",
    "        job_extra_directives = instance.get(\"job_extra_directives\", [])\n",
    "        job_extra_directives += [\n",
    "            f\"--output={logs_dir}/dask_job_%j.out\",\n",
    "            f\"--error={logs_dir}/dask_job_%j.err\"\n",
    "        ]\n",
    "        cluster = SLURMCluster(\n",
    "            interface=\"ib0\",\n",
    "            queue=instance[\"queue\"],\n",
    "            cores=instance[\"cores\"],\n",
    "            processes=instance[\"processes\"],\n",
    "            memory=instance[\"memory\"],\n",
    "            job_extra_directives=job_extra_directives,\n",
    "        )\n",
    "        cluster.adapt(**adapt)\n",
    "        return Client(cluster)\n",
    "    else:\n",
    "        return Client()\n",
    "\n",
    "def import_catalog(entry, artifact_name, output_path, client):\n",
    "    input_file_list = entry.get(\"input_file_list\")\n",
    "    if input_file_list is None:\n",
    "        input_file_list = [entry[\"path\"]]\n",
    "\n",
    "    df = dd.read_parquet(input_file_list)\n",
    "\n",
    "    has_survey = \"survey\" in df.columns\n",
    "    has_year = \"year\" in df.columns\n",
    "\n",
    "    if not has_survey and \"survey\" in entry:\n",
    "        df[\"survey\"] = entry[\"survey\"]\n",
    "        has_survey = True\n",
    "\n",
    "    if not has_year and \"year\" in entry:\n",
    "        df[\"year\"] = entry[\"year\"]\n",
    "        has_year = True\n",
    "\n",
    "    # If metadata is already present, use it directly\n",
    "    if has_survey and has_year and \"input_file_list\" in entry:\n",
    "        column_names = list(entry[\"columns\"].values()) + [\"survey\", \"year\"]\n",
    "        file_reader = ParquetReader(column_names=column_names)\n",
    "        args = ImportArguments(\n",
    "            ra_column=entry[\"columns\"][\"ra\"],\n",
    "            dec_column=entry[\"columns\"][\"dec\"],\n",
    "            input_file_list=input_file_list,\n",
    "            file_reader=file_reader,\n",
    "            output_artifact_name=artifact_name,\n",
    "            output_path=output_path,\n",
    "        )\n",
    "    else:\n",
    "        # Otherwise, save a temporary version with added metadata\n",
    "        temp_parquet_path = os.path.join(output_path, f\"tmp_{artifact_name}_with_meta\")\n",
    "        df.to_parquet(temp_parquet_path, engine=\"pyarrow\", write_index=False)\n",
    "        column_names = list(entry[\"columns\"].values())\n",
    "        if has_survey:\n",
    "            column_names.append(\"survey\")\n",
    "        if has_year:\n",
    "            column_names.append(\"year\")\n",
    "        file_reader = ParquetReader(column_names=column_names)\n",
    "        args = ImportArguments(\n",
    "            ra_column=entry[\"columns\"][\"ra\"],\n",
    "            dec_column=entry[\"columns\"][\"dec\"],\n",
    "            input_file_list=sorted(glob.glob(os.path.join(temp_parquet_path, \"part.*.parquet\"))),\n",
    "            file_reader=file_reader,\n",
    "            output_artifact_name=artifact_name,\n",
    "            output_path=output_path,\n",
    "        )\n",
    "    pipeline_with_client(args, client)\n",
    "\n",
    "def generate_margin_cache(hats_path, output_path, artifact_name, client):\n",
    "    catalog = hats.read_hats(hats_path)\n",
    "    info = catalog.partition_info.as_dataframe().astype(int)\n",
    "    if len(info) > 1:\n",
    "        args = MarginCacheArguments(\n",
    "            input_catalog_path=hats_path,\n",
    "            output_path=output_path,\n",
    "            margin_threshold=1.0,\n",
    "            output_artifact_name=artifact_name\n",
    "        )\n",
    "        pipeline_with_client(args, client)\n",
    "    else:\n",
    "        warnings.warn(f\"Number of pixels is {len(info)}. Margin cache will not be generated.\")\n",
    "\n",
    "def crossmatch_and_merge(left_catalog, right_catalog, output_xmatch_dir):\n",
    "    # Perform 1 arcsec crossmatch between left and right catalogs\n",
    "    xmatched = left_catalog.crossmatch(right_catalog, radius_arcsec=1.0, n_neighbors=1, suffixes=(\"left\", \"right\"))\n",
    "    xmatched.to_hats(output_xmatch_dir, overwrite=True)\n",
    "    df = lsdb.read_hats(output_xmatch_dir)._ddf\n",
    "\n",
    "    # Define rule to decide which source to keep\n",
    "    def decide_winner(row):\n",
    "        zf1, zf2 = row[\"z_flagleft\"], row[\"z_flagright\"]\n",
    "        y1, y2 = row[\"yearleft\"], row[\"yearright\"]\n",
    "        if zf1 > zf2: return \"left\"\n",
    "        elif zf1 < zf2: return \"right\"\n",
    "        return \"left\" if y1 > y2 else \"right\" if y1 < y2 else \"tie\"\n",
    "\n",
    "    df = df.assign(winner=df.map_partitions(lambda p: p.apply(decide_winner, axis=1), meta=(\"winner\", \"str\")))\n",
    "\n",
    "    # Extract IDs of the discarded sources\n",
    "    def extract_losers(part):\n",
    "        return pd.DataFrame({\n",
    "            \"loser_cat1\": part.loc[part[\"winner\"] == \"right\", \"idleft\"],\n",
    "            \"loser_cat2\": part.loc[part[\"winner\"] == \"left\", \"idright\"]\n",
    "        })\n",
    "\n",
    "    losers_df = df.map_partitions(extract_losers).compute()\n",
    "    loser_ids_cat1 = list(losers_df[\"loser_cat1\"].dropna().unique())\n",
    "    loser_ids_cat2 = list(losers_df[\"loser_cat2\"].dropna().unique())\n",
    "\n",
    "    # Remove discarded sources and merge the rest\n",
    "    filtered_left = left_catalog[~left_catalog[\"id\"].isin(loser_ids_cat1)]\n",
    "    filtered_right = right_catalog[~right_catalog[\"id\"].isin(loser_ids_cat2)]\n",
    "    final_catalog = filtered_left.merge(filtered_right, how=\"outer\", on=\"id\", suffixes=(\"_left\", \"_right\"))\n",
    "\n",
    "    # Collapse columns using priority from left to right\n",
    "    return final_catalog.assign(\n",
    "        ra=final_catalog[\"ra_left\"].combine_first(final_catalog[\"ra_right\"]),\n",
    "        dec=final_catalog[\"dec_left\"].combine_first(final_catalog[\"dec_right\"]),\n",
    "        z=final_catalog[\"z_left\"].combine_first(final_catalog[\"z_right\"]),\n",
    "        z_flag=final_catalog[\"z_flag_left\"].combine_first(final_catalog[\"z_flag_right\"]),\n",
    "        survey=final_catalog[\"survey_left\"].combine_first(final_catalog[\"survey_right\"]),\n",
    "        year=final_catalog[\"year_left\"].combine_first(final_catalog[\"year_right\"]),\n",
    "    )[[\"id\", \"ra\", \"dec\", \"z\", \"z_flag\", \"survey\", \"year\"]]\n",
    "\n",
    "# ----------------------------\n",
    "# PIPELINE EXECUTION FOR N CATALOGS\n",
    "# ----------------------------\n",
    "\n",
    "client = setup_cluster(config, logs_dir)\n",
    "\n",
    "# Step 1: import the first catalog\n",
    "import_catalog(catalogs[0], f\"cat0_hats\", temp_dir, client)\n",
    "cat_prev = lsdb.read_hats(os.path.join(temp_dir, f\"cat0_hats\"))\n",
    "\n",
    "# Step 2: sequential crossmatch and merge for the remaining catalogs\n",
    "for i in range(1, len(catalogs)):\n",
    "    import_catalog(catalogs[i], f\"cat{i}_hats\", temp_dir, client)\n",
    "    generate_margin_cache(os.path.join(temp_dir, f\"cat{i}_hats\"), temp_dir, f\"cat{i}_margin\", client)\n",
    "    cat_curr = lsdb.read_hats(\n",
    "        os.path.join(temp_dir, f\"cat{i}_hats\"),\n",
    "        margin_cache=os.path.join(temp_dir, f\"cat{i}_margin\")\n",
    "    )\n",
    "    cat_merged = crossmatch_and_merge(cat_prev, cat_curr, os.path.join(temp_dir, f\"xmatch_{i}\"))\n",
    "    \n",
    "    # Save intermediate collapsed version\n",
    "    collapsed_path = os.path.join(temp_dir, f\"collapsed_{i}\")\n",
    "    cat_merged.to_parquet(collapsed_path, by_layer=True)\n",
    "\n",
    "    # Re-import as HATS to continue processing\n",
    "    import_catalog(\n",
    "        {\n",
    "            \"input_file_list\": sorted(glob.glob(os.path.join(collapsed_path, \"base\", \"part.*.parquet\"))),\n",
    "            \"file_reader\": \"parquet\",\n",
    "            \"columns\": {\n",
    "                \"id\": \"id\", \"ra\": \"ra\", \"dec\": \"dec\",\n",
    "                \"z\": \"z\", \"z_flag\": \"z_flag\", \"survey\": \"survey\", \"year\": \"year\"\n",
    "            },\n",
    "            \"ra\": \"ra\", \"dec\": \"dec\"\n",
    "        },\n",
    "        f\"collapsed_{i}_hats\", temp_dir, client\n",
    "    )\n",
    "    cat_prev = lsdb.read_hats(os.path.join(temp_dir, f\"collapsed_{i}_hats\"))\n",
    "\n",
    "# ----------------------------\n",
    "# SAVE FINAL COLLAPSED PARQUET\n",
    "# ----------------------------\n",
    "\n",
    "final_collapsed_dir = os.path.join(temp_dir, f\"collapsed_{len(catalogs) - 1}\")\n",
    "final_output_path = os.path.join(output_dir, f\"{config['output_name']}.parquet\")\n",
    "final_parts = sorted(glob.glob(os.path.join(final_collapsed_dir, \"base\", \"part.*.parquet\")))\n",
    "\n",
    "print(f\"Saving single .parquet file to: {final_output_path}\")\n",
    "\n",
    "# Load by-layer parts into a single pandas DataFrame\n",
    "df_final = dd.read_parquet(final_parts).compute()\n",
    "\n",
    "# Save single-file Parquet (not directory)\n",
    "df_final.to_parquet(\n",
    "    final_output_path,\n",
    "    engine=\"pyarrow\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Pipeline completed successfully.\")\n",
    "\n",
    "# ----------------------------\n",
    "# REMOVE TEMP DIRECTORY\n",
    "# ----------------------------\n",
    "\n",
    "delete_temp_in_the_end = False  # Set to True to remove temp folder after run\n",
    "\n",
    "if delete_temp_in_the_end:\n",
    "    print(f\"Removing temporary directory: {temp_dir}\")\n",
    "    import shutil\n",
    "    shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "    print(\"Temporary directory removed.\")\n",
    "else:\n",
    "    print(f\"Temporary directory kept at: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab2011-975b-478b-b0a5-59de97468989",
   "metadata": {},
   "source": [
    "# Validation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc28e3-23ed-4a38-a537-dcd651e07c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_xmatch_1 = \"/scratch/users/luigi.silva/pzserver_pipelines/combine_specz/notebooks/process001/temp/xmatch_1\"\n",
    "path_to_collapsed_1 = \"/scratch/users/luigi.silva/pzserver_pipelines/combine_specz/notebooks/process001/temp/collapsed_1/base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3a43e-ac23-44a2-9b18-b3cdd229fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Dask DataFrame from the crossmatch result\n",
    "df_xmatch_1 = lsdb.read_hats(path_to_xmatch_1)\n",
    "df_xmatch_1 = df_xmatch_1._ddf  # Access underlying Dask DataFrame\n",
    "\n",
    "# Optional: trigger full computation to inspect the crossmatch result\n",
    "df_xmatch_1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9e558-c2e8-4cca-b408-6f1216f62f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the collapsed catalog (already deduplicated) as a Pandas DataFrame\n",
    "df_collapsed_1 = pd.read_parquet(path_to_collapsed_1)\n",
    "\n",
    "# Preview contents of collapsed catalog\n",
    "df_collapsed_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1face-fabe-477b-ac3d-64c0605ac56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract unique IDs from both sides of the crossmatch result\n",
    "ids_cat_left = df_xmatch_1[\"idleft\"].dropna().unique().compute()\n",
    "ids_cat_right = df_xmatch_1[\"idright\"].dropna().unique().compute()\n",
    "\n",
    "# 3. Combine both sets of IDs into one unified set\n",
    "ids_from_crossmatch = set(ids_cat_left.tolist()) | set(ids_cat_right.tolist())\n",
    "\n",
    "# 4. Filter the collapsed final DataFrame to include only crossmatched objects\n",
    "df_final_subset = df_collapsed_1[df_collapsed_1[\"id\"].isin(ids_from_crossmatch)]\n",
    "\n",
    "# 5. Display the filtered result (crossmatched objects only)\n",
    "df_final_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6026dc27-21a0-4454-ac67-67ab520dcce0",
   "metadata": {},
   "source": [
    "# Validation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c8905-4926-4740-b15f-4ebd88e6846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_xmatch_2 = \"/scratch/users/luigi.silva/pzserver_pipelines/combine_specz/notebooks/process001/temp/xmatch_2\"\n",
    "path_to_collapsed_final = final_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfc241-0cd4-402f-86e1-9df6d127fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Dask DataFrame from the second crossmatch result\n",
    "df_xmatch_2 = lsdb.read_hats(path_to_xmatch_2)\n",
    "df_xmatch_2 = df_xmatch_2._ddf  # Access the underlying Dask DataFrame\n",
    "\n",
    "# Optional: compute the crossmatch result to inspect it fully\n",
    "df_xmatch_2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad71aee-1de6-4d8a-88f4-e01fb77efdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final collapsed catalog (after all merges) as a Pandas DataFrame\n",
    "df_collapsed_final = pd.read_parquet(path_to_collapsed_final)\n",
    "\n",
    "# Display the final merged catalog\n",
    "df_collapsed_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac51b7b-4f28-4d53-bf06-4f812ec5e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Extract unique source IDs from both left and right columns of the crossmatch\n",
    "ids_cat_left = df_xmatch_2[\"idleft\"].dropna().unique().compute()\n",
    "ids_cat_right = df_xmatch_2[\"idright\"].dropna().unique().compute()\n",
    "\n",
    "# 3. Combine both arrays into a single set of matched IDs\n",
    "ids_from_crossmatch = set(ids_cat_left.tolist()) | set(ids_cat_right.tolist())\n",
    "\n",
    "# 4. Filter the final catalog to include only the matched sources\n",
    "df_final_subset = df_collapsed_final[df_collapsed_final[\"id\"].isin(ids_from_crossmatch)]\n",
    "\n",
    "# 5. Display the filtered DataFrame (objects involved in the last crossmatch)\n",
    "df_final_subset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipe_csc",
   "language": "python",
   "name": "pipe_csc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
