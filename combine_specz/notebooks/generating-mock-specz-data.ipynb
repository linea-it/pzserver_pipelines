{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f54300-3d45-4a8e-a2ac-977991d5782e",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a3b32-ee7a-4487-8ccb-7956032b1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize random generator with fixed seed for reproducibility\n",
    "rng = np.random.default_rng(seed=123)\n",
    "\n",
    "# Sizes of the mock catalogs\n",
    "n1, n2, n3 = 100000, 200000, 300000\n",
    "# n1, n2, n3 = 30, 40, 50  # Uncomment for testing with smaller datasets\n",
    "\n",
    "# Generate random RA/DEC coordinates\n",
    "def random_coords(n):\n",
    "    return rng.uniform(0, 360, n), rng.uniform(-90, 90, n)\n",
    "\n",
    "# Apply small random offset to simulate measurement uncertainty (~0.25 arcsec)\n",
    "def add_noise(ra, dec):\n",
    "    delta_ra = rng.uniform(-0.00007, 0.00007)\n",
    "    delta_dec = rng.uniform(-0.00007, 0.00007)\n",
    "    return ra + delta_ra, dec + delta_dec\n",
    "\n",
    "# --- 1. Objects common to all three catalogs ---\n",
    "ra_common3, dec_common3 = random_coords(5)\n",
    "z_common3 = rng.uniform(0.1, 1.0, 5)\n",
    "flags_common3 = [(4,3,3), (4,4,3), (3,4,3), (3,4,4), (3,3,4)]  # Different z_flags in each catalog\n",
    "\n",
    "# --- 2. Objects shared only between catalog 1 and 2 ---\n",
    "ra_common12_extra, dec_common12_extra = random_coords(5)\n",
    "z_common12_extra = rng.uniform(0.1, 1.0, 5)\n",
    "flags_common12_extra = [(3,4), (4,3), (3,3), (4,4), (3,4)]\n",
    "\n",
    "# --- 3. Objects shared only between catalog 2 and 3 ---\n",
    "ra_common23_extra, dec_common23_extra = random_coords(2)\n",
    "z_common23_extra = rng.uniform(0.1, 1.0, 2)\n",
    "flags_common23_extra = [(3,3), (4,3)]\n",
    "\n",
    "# --- 4. Objects shared only between catalog 1 and 3 ---\n",
    "ra_common13_extra, dec_common13_extra = random_coords(3)\n",
    "z_common13_extra = rng.uniform(0.1, 1.0, 3)\n",
    "flags_common13_extra = [(3,4), (3,3), (4,3)]\n",
    "\n",
    "# Generate formatted IDs for each catalog\n",
    "def gen_ids(prefix, start, count):\n",
    "    return [f\"{prefix}_id_{i:07d}\" for i in range(start, start + count)]\n",
    "\n",
    "# Initialize lists to store catalog rows\n",
    "rows1, rows2, rows3 = [], [], []\n",
    "\n",
    "# Add common objects to all three catalogs with small positional/z shifts and different z_flags\n",
    "for i in range(5):\n",
    "    ra, dec = ra_common3[i], dec_common3[i]\n",
    "    rows1.append((f\"c1_id_common3_{i}\", ra, dec, z_common3[i], flags_common3[i][0]))\n",
    "    ra2, dec2 = add_noise(ra, dec)\n",
    "    rows2.append((f\"c2_id_common3_{i}\", ra2, dec2, z_common3[i] + 0.0001, flags_common3[i][1]))\n",
    "    ra3, dec3 = add_noise(ra, dec)\n",
    "    rows3.append((f\"c3_id_common3_{i}\", ra3, dec3, z_common3[i] + 0.0002, flags_common3[i][2]))\n",
    "\n",
    "# Add objects shared between catalog 1 and 2 only\n",
    "for i in range(5):\n",
    "    ra, dec = ra_common12_extra[i], dec_common12_extra[i]\n",
    "    rows1.append((f\"c1_id_common12_{i}\", ra, dec, z_common12_extra[i], flags_common12_extra[i][0]))\n",
    "    ra2, dec2 = add_noise(ra, dec)\n",
    "    rows2.append((f\"c2_id_common12_{i}\", ra2, dec2, z_common12_extra[i] + 0.0001, flags_common12_extra[i][1]))\n",
    "\n",
    "# Add objects shared between catalog 2 and 3 only\n",
    "for i in range(2):\n",
    "    ra, dec = ra_common23_extra[i], dec_common23_extra[i]\n",
    "    rows2.append((f\"c2_id_common23_{i}\", ra, dec, z_common23_extra[i], flags_common23_extra[i][0]))\n",
    "    ra3, dec3 = add_noise(ra, dec)\n",
    "    rows3.append((f\"c3_id_common23_{i}\", ra3, dec3, z_common23_extra[i] + 0.0001, flags_common23_extra[i][1]))\n",
    "\n",
    "# Add objects shared between catalog 1 and 3 only\n",
    "for i in range(3):\n",
    "    ra, dec = ra_common13_extra[i], dec_common13_extra[i]\n",
    "    rows1.append((f\"c1_id_common13_{i}\", ra, dec, z_common13_extra[i], flags_common13_extra[i][0]))\n",
    "    ra3, dec3 = add_noise(ra, dec)\n",
    "    rows3.append((f\"c3_id_common13_{i}\", ra3, dec3, z_common13_extra[i] + 0.0001, flags_common13_extra[i][1]))\n",
    "\n",
    "# Count how many rows are already added (used to avoid ID collisions)\n",
    "count1 = len(rows1)\n",
    "count2 = len(rows2)\n",
    "count3 = len(rows3)\n",
    "\n",
    "# Fill remaining rows with randomly generated unique objects\n",
    "def fill_random(rows, target, prefix, start_idx):\n",
    "    needed = target - len(rows)\n",
    "    ras, decs = random_coords(needed)\n",
    "    zs = rng.uniform(0.1, 1.0, needed)\n",
    "    flags = rng.choice([3, 4], size=needed)\n",
    "    ids = gen_ids(prefix, start_idx, needed)\n",
    "    for i in range(needed):\n",
    "        rows.append((ids[i], ras[i], decs[i], zs[i], flags[i]))\n",
    "\n",
    "fill_random(rows1, n1, \"c1\", count1)\n",
    "fill_random(rows2, n2, \"c2\", count2)\n",
    "fill_random(rows3, n3, \"c3\", count3)\n",
    "\n",
    "# Create Pandas DataFrames for each catalog\n",
    "df1 = pd.DataFrame(rows1, columns=[\"id\", \"ra\", \"dec\", \"z\", \"z_flag\"])\n",
    "df2 = pd.DataFrame(rows2, columns=[\"id\", \"ra\", \"dec\", \"z\", \"z_flag\"])\n",
    "df3 = pd.DataFrame(rows3, columns=[\"id\", \"ra\", \"dec\", \"z\", \"z_flag\"])\n",
    "\n",
    "# Save catalogs to Parquet files\n",
    "df1.to_parquet(\"mock-specz-1.parquet\", index=False)\n",
    "df2.to_parquet(\"mock-specz-2.parquet\", index=False)\n",
    "df3.to_parquet(\"mock-specz-3.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed729d93-d6e6-4718-b108-9e574d5ba3d1",
   "metadata": {},
   "source": [
    "# Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e45c44-fd1d-433c-856d-ec781cc55365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Load the catalogs ---\n",
    "df1 = pd.read_parquet(\"mock-specz-1.parquet\")\n",
    "df2 = pd.read_parquet(\"mock-specz-2.parquet\")\n",
    "df3 = pd.read_parquet(\"mock-specz-3.parquet\")\n",
    "\n",
    "# --- 2. Convert RA/DEC to SkyCoord objects for spherical matching ---\n",
    "coords1 = SkyCoord(ra=df1[\"ra\"].values * u.deg, dec=df1[\"dec\"].values * u.deg)\n",
    "coords2 = SkyCoord(ra=df2[\"ra\"].values * u.deg, dec=df2[\"dec\"].values * u.deg)\n",
    "coords3 = SkyCoord(ra=df3[\"ra\"].values * u.deg, dec=df3[\"dec\"].values * u.deg)\n",
    "\n",
    "# --- 3. Function to perform crossmatch between two coordinate lists ---\n",
    "def crossmatch(coords_a, coords_b, max_sep=1 * u.arcsec):\n",
    "    idx_a, idx_b, d2d, _ = coords_a.search_around_sky(coords_b, max_sep)\n",
    "    return idx_b, idx_a  # Return in order: index in A, index in B\n",
    "\n",
    "# Perform pairwise crossmatches\n",
    "# Match between catalog 1 and 2\n",
    "idx1_12, idx2_12 = crossmatch(coords1, coords2)\n",
    "match12 = pd.DataFrame({\n",
    "    \"id1\": df1.iloc[idx1_12][\"id\"].values,\n",
    "    \"id2\": df2.iloc[idx2_12][\"id\"].values,\n",
    "})\n",
    "\n",
    "# Match between catalog 2 and 3\n",
    "idx2_23, idx3_23 = crossmatch(coords2, coords3)\n",
    "match23 = pd.DataFrame({\n",
    "    \"id2\": df2.iloc[idx2_23][\"id\"].values,\n",
    "    \"id3\": df3.iloc[idx3_23][\"id\"].values,\n",
    "})\n",
    "\n",
    "# Match between catalog 1 and 3\n",
    "idx1_13, idx3_13 = crossmatch(coords1, coords3)\n",
    "match13 = pd.DataFrame({\n",
    "    \"id1\": df1.iloc[idx1_13][\"id\"].values,\n",
    "    \"id3\": df3.iloc[idx3_13][\"id\"].values,\n",
    "})\n",
    "\n",
    "# --- 4. Identify triple matches (objects matched across all three catalogs) ---\n",
    "# Merge matches between 1–2 and 2–3 on id2\n",
    "match12_23 = pd.merge(match12, match23, on=\"id2\", how=\"inner\")\n",
    "# Further merge with 1–3 matches to ensure full 3-way connection (id1–id3 also match)\n",
    "match123 = pd.merge(match12_23, match13, on=[\"id1\", \"id3\"], how=\"inner\")\n",
    "\n",
    "# --- 5. Print basic statistics ---\n",
    "print(f\"Match 1–2: {len(match12)} pairs\")\n",
    "print(f\"Match 2–3: {len(match23)} pairs\")\n",
    "print(f\"Match 1–3: {len(match13)} pairs\")\n",
    "print(f\"Match 1–2–3: {len(match123)} triplets\")\n",
    "\n",
    "# --- 6. Validate matched IDs against expected test objects (catalog 1 perspective) ---\n",
    "# Expected IDs for 1–2 matches (5 shared in all three + 5 exclusive to 1–2)\n",
    "expected_12_ids = [f\"c1_id_common3_{i}\" for i in range(5)] + [f\"c1_id_common12_{i}\" for i in range(5)]\n",
    "expected_12_ids_set = set(expected_12_ids)\n",
    "\n",
    "# Extract unique IDs from match12\n",
    "matched_1_ids = set(match12[\"id1\"].values)\n",
    "\n",
    "# Identify unexpected IDs (false positives)\n",
    "extra_1_ids = matched_1_ids - expected_12_ids_set\n",
    "if extra_1_ids:\n",
    "    print(f\"⚠️ Unexpected objects in match 1–2 (id1): {extra_1_ids}\")\n",
    "\n",
    "# --- 7. Annotate which matches are expected ---\n",
    "match12[\"is_expected\"] = match12[\"id1\"].isin(expected_12_ids_set)\n",
    "\n",
    "# --- 8. Join original catalog rows for detailed inspection ---\n",
    "df_inspect_12 = match12.merge(df1, left_on=\"id1\", right_on=\"id\", suffixes=(\"\", \"_cat1\"))\n",
    "df_inspect_12 = df_inspect_12.merge(df2, left_on=\"id2\", right_on=\"id\", suffixes=(\"_cat1\", \"_cat2\"))\n",
    "\n",
    "# --- 9. Select relevant columns to reduce visual clutter ---\n",
    "df_inspect_12 = df_inspect_12[[\"id1\", \"ra_cat1\", \"dec_cat1\", \"z_cat1\", \"z_flag_cat1\",\n",
    "                               \"id2\", \"ra_cat2\", \"dec_cat2\", \"z_cat2\", \"z_flag_cat2\",\n",
    "                               \"is_expected\"]]\n",
    "\n",
    "# --- 10. Show matches with unexpected ones first ---\n",
    "pd.set_option(\"display.max_rows\", None)  # Optional: show all rows in notebook\n",
    "df_inspect_12_sorted = df_inspect_12.sort_values(by=\"is_expected\")\n",
    "\n",
    "# Display the sorted result\n",
    "df_inspect_12_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipe_csc",
   "language": "python",
   "name": "pipe_csc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
