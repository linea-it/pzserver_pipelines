#!/usr/bin/env python3

import argparse
import datetime
import os
import time
from pathlib import Path

from dask.distributed import Client
from executor import get_executor
from product_handle import create_output
from specz import Specz
from utils import dump_yml, load_yml, setup_logger


class Runner:

    def __init__(self, config, cwd=".") -> None:
        self.logger = setup_logger("csc", logdir=cwd)
        self.cwd = cwd
        self.process_info_path = Path(self.cwd, "process.yml")
        self.process_info = None
        self.start_time = None
        self.end_time = None

        # Loading config
        pipe_config = load_yml(config)

        # Define output dir and output name
        self.output_name = pipe_config.get("output_name", "csc.parquet")
        self.output_format = pipe_config.get("output_format", None)
        self.output_root_dir = Path(pipe_config.get("output_root_dir", "."))

        if not self.output_root_dir.is_absolute():
            self.output_root_dir = Path(self.cwd, self.output_root_dir)

        if not self.output_format:
            self.output_format = "parquet"

        self.output_dir = Path(pipe_config.get("output_dir", "out"))

        # Create output dir
        output_full_dir = Path(self.output_root_dir, self.output_dir)
        os.makedirs(output_full_dir, exist_ok=True)

        # Loading inputs
        self.inputs = pipe_config.get("inputs")
        self.logger.info("Inputs: %s", self.inputs)

        # Loading parameters
        self.param = pipe_config.get("param", {})
        self.logger.info("Params: %s", self.param)

        # Set executors
        self.executors = pipe_config.get("executor")

        # Creating process info yaml
        if not self.process_info_path.is_file():
            self.process_info_path.touch()

        # Loading process info yaml
        process_info = load_yml(self.process_info_path)
        if not process_info:
            process_info = {}
        self.process_info = process_info

        self.cluster = get_executor(self.executors)

    def __enter__(self):
        return self

    def add_info(self, key, value):
        """Add info in process.yaml"""
        self.process_info[key] = value
        dump_yml(self.process_info_path, self.process_info)

    def run(self):
        """Run Combine Specz Catalog"""

        # Adding start time
        self.start_time = time.time()
        self.add_info("start_time", datetime.datetime.now())

        specz = self.inputs.get("specz")
        self.logger.info(f"Number of selected catalogs: {len(specz)}")

        if len(specz) < 2:
            self.logger.exception(
                "There must be at least 2 catalogs to run the pipeline."
            )

        with Client(self.cluster) as client:
            specz = Specz(specz, client)

            outputfile = create_output(
                specz.dataframe,
                self.output_root_dir,
                self.output_dir,
                self.output_name,
                self.output_format,
            )

            self.__register_outputs(
                self.output_root_dir, outputfile, {"ra": "ra", "dec": "dec", "z": "z"}
            )

            self.logger.info(f"--> Object Count: {len(specz.dataframe)}")

        self.end_time = time.time() - self.start_time
        self.logger.info("Time elapsed: %s", str(self.end_time))

    def __register_outputs(self, root_dir, filepath, assoc, role="main"):
        """Register outputs in process.yml

        Args:
            root_dir (str): output root dir
            filepath (str): output path
            assoc (dic): columns association
            role (str, optional): role name. Defaults to 'main'.
        """

        outpath = str(Path(filepath).resolve())
        outputs = self.process_info.get("outputs", [])
        outputs.append(
            {
                "path": outpath,
                "root_dir": root_dir,
                "role": role,
                "columns_assoc": assoc,
            }
        )
        self.add_info("outputs", outputs)

    def __exit__(self, exc_type, exc_value, traceback):

        self.cluster.close()
        self.add_info("end_time", datetime.datetime.now())

        if exc_type:
            self.logger.error("%s: %s", exc_type.__name__, exc_value)
            self.logger.debug("Traceback: %s", traceback)
            self.add_info("status", "Failed")
        else:
            self.add_info("status", "Successful")


if __name__ == "__main__":
    # Create the parser and add arguments
    parser = argparse.ArgumentParser()
    parser.add_argument(dest="config_path", help="yaml config path")
    parser.add_argument(
        dest="cwd", nargs="?", help="processing dir", default=os.getcwd()
    )

    args = parser.parse_args()
    config_path = args.config_path
    _cwd = args.cwd

    # Run pipeline
    with Runner(config_path, _cwd) as cscrun:
        cscrun.run()
