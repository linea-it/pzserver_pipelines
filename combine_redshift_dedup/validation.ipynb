{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d2c6aa-e836-4c37-b67f-b8a828c80590",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b9448-1cdf-46a1-b07c-b031c3c5f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "user = getpass.getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921fa07e-c9e5-4ff5-bfa9-ff27115717d6",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66b946-64d0-4182-aca2-1268f6b3a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# CONFIGURATION SECTION\n",
    "# =========================================================\n",
    "#input_paths = [\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/2dfgrs_final_release.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/2dflens_final_release.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/2mrs_v240.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/3dhst_v4.1.5.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/6dfgs_dr3.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/astrodeep_jwst.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/astrodeep-gs43.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/desi_dr1_in_lsst_dp1_fields.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/jades_dr3.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/mosdef_final_release.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/ozdes_dr2.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/primus_dr1.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/vandels_dr4.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/vlt_vimos_v2.0.1.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/vuds_dr1.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/vvds_final_release.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/johns-catalogs/z_cat_CANDELS_clean_sitcomtn-154.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/johns-catalogs/z_cat_NED_clean_sitcomtn-154.parquet\",\n",
    "#]\n",
    "\n",
    "input_paths = glob.glob('test_data/*.parquet')\n",
    "\n",
    "# replace paths in case of using local env \n",
    "final_catalog_path = f\"./process001/outputs/crd.parquet\"\n",
    "prepared_temp_dir = f\"./process001/temp/\"\n",
    "\n",
    "combine_mode = \"concatenate_and_mark_duplicates\" # Options: \"concatenate\", \"concatenate_and_mark_duplicates\", or \"concatenate_and_remove_duplicates\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1390e-4d78-46dc-a879-93741200e42d",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d742f-2815-4c78-b471-ed538be842b7",
   "metadata": {},
   "source": [
    "## Basic Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2c2af-86a5-4c2c-abfb-3f304ed3b90d",
   "metadata": {},
   "source": [
    "Counting input and output rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0ff59-ef63-4ff9-886f-37606a01cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# COUNT INPUT ROWS\n",
    "# =========================================================\n",
    "total_input_rows = 0\n",
    "for path in input_paths:\n",
    "    if os.path.exists(path):\n",
    "        parquet_file = pq.ParquetFile(path)\n",
    "        n_rows = parquet_file.metadata.num_rows\n",
    "        print(f\"{path} -> {n_rows} rows\")\n",
    "        total_input_rows += n_rows\n",
    "    else:\n",
    "        warnings.warn(f\"‚ö†Ô∏è File not found: {path}\")\n",
    "\n",
    "print(f\"‚úÖ Total number of input rows: {total_input_rows}\")\n",
    "\n",
    "# =========================================================\n",
    "# LOAD FINAL MERGED CATALOG\n",
    "# =========================================================\n",
    "if not os.path.exists(final_catalog_path):\n",
    "    raise FileNotFoundError(f\"‚ùå Final catalog not found: {final_catalog_path}\")\n",
    "\n",
    "df_final = pd.read_parquet(final_catalog_path)\n",
    "print(f\"‚úÖ Total number of rows in final catalog: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0918b-8f97-44f5-bf8c-2cb41ba2b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a85f2-2deb-419b-84f0-893ccbf4ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9d0d0-c23e-41d4-a01c-3e53161f01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db02aa8-5dea-41c3-9807-a18308f4b93e",
   "metadata": {},
   "source": [
    "Basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff02fa-50be-4d05-ae3b-d62ae76df68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f2bdc-55a9-463c-859b-9dbb1ecd1e57",
   "metadata": {},
   "source": [
    "Counting tie_result values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf326961-1cf0-4587-a47c-bbec4a99dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if combine_mode != \"concatenate\":\n",
    "    print(df_final[\"tie_result\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd6ce4-6d28-484f-bf65-68b44085a852",
   "metadata": {},
   "source": [
    "Counting survey values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2a90c-386c-4904-824a-547509576407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e8dd0-f6cc-4310-a896-fba216515967",
   "metadata": {},
   "source": [
    "Checking the percentage of unsolved objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d2df1-3df5-45a3-a1bb-217a2925c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "if combine_mode != \"concatenate\":\n",
    "    # Total number of objects\n",
    "    total_all = len(df_final)\n",
    "    \n",
    "    # Filter objects that were compared (compared_to is not null or empty)\n",
    "    mask_compared = df_final[\"compared_to\"].notna() & (df_final[\"compared_to\"] != \"\")\n",
    "    df_compared = df_final[mask_compared]\n",
    "    \n",
    "    # Count how many have tie_result == 2\n",
    "    count_tie2 = (df_final[\"tie_result\"] == 2).sum()\n",
    "    count_tie2_compared = (df_compared[\"tie_result\"] == 2).sum()\n",
    "    \n",
    "    # Percentages\n",
    "    percent_all = (count_tie2 / total_all) * 100 if total_all > 0 else 0\n",
    "    percent_compared = (count_tie2_compared / len(df_compared)) * 100 if len(df_compared) > 0 else 0\n",
    "    \n",
    "    # Formatted print\n",
    "    print(f\"üìä tie_result == 2 represents:\")\n",
    "    print(f\"  ‚Ä¢ {percent_all:.2f}% of the total ({count_tie2} out of {total_all})\")\n",
    "    print(f\"  ‚Ä¢ {percent_compared:.2f}% of the compared objects ({count_tie2_compared} out of {len(df_compared)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d3621-1cf4-4aa1-94f8-6a6f7dfb5985",
   "metadata": {},
   "source": [
    "## Automatic Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e8443-3d0c-4863-8875-b5da0eac56fd",
   "metadata": {},
   "source": [
    "All rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6eb193-4c89-4359-9a88-a8123d892fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validator_crc.py\n",
    "from __future__ import annotations\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Utilities & normalization\n",
    "# ------------------------------\n",
    "\n",
    "TYPE_PRIORITY = {\"s\": 3, \"g\": 2, \"p\": 1}  # others/NA -> 0\n",
    "\n",
    "\n",
    "def _norm_str(s):\n",
    "    \"\"\"Normalize string-like fields; returns None if empty-like.\"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    if not s or s.lower() in {\"na\", \"nan\", \"<na>\", \"none\", \"null\"}:\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "\n",
    "def _type_score(t):\n",
    "    \"\"\"Map instrument_type_homogenized to numeric priority.\"\"\"\n",
    "    t = _norm_str(t)\n",
    "    return TYPE_PRIORITY.get(t, 0)\n",
    "\n",
    "\n",
    "def _to_num(x):\n",
    "    \"\"\"Best-effort numeric cast -> float; returns np.nan on failure.\"\"\"\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def _z_diff(a, b):\n",
    "    \"\"\"Absolute z difference; returns np.inf if either is NA (undefined).\"\"\"\n",
    "    if a is None or b is None:\n",
    "        return np.inf\n",
    "    if (isinstance(a, float) and math.isnan(a)) or (isinstance(b, float) and math.isnan(b)):\n",
    "        return np.inf\n",
    "    return abs(a - b)\n",
    "\n",
    "\n",
    "def _is_na(x):\n",
    "    return x is None or (isinstance(x, float) and math.isnan(x))\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Graph building from compared_to  (EXCLUDES STARS)\n",
    "# ------------------------------\n",
    "\n",
    "def build_components(df: pd.DataFrame, excluded_ids: set[str] | None = None) -> list[tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    Build undirected components using 'CRD_ID' and 'compared_to' comma-list.\n",
    "\n",
    "    - Only rows with non-empty compared_to contribute edges.\n",
    "    - Any edge touching IDs in `excluded_ids` (e.g., stars) is dropped.\n",
    "    - Nodes in `excluded_ids` are not included in the components.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with columns CRD_ID, compared_to.\n",
    "        excluded_ids: Optional set of IDs to exclude from edges/nodes.\n",
    "\n",
    "    Returns:\n",
    "        List of sorted tuples, one per connected component.\n",
    "    \"\"\"\n",
    "    excluded_ids = excluded_ids or set()\n",
    "\n",
    "    edges = []\n",
    "    seen_nodes = set()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        crd = _norm_str(row.get(\"CRD_ID\"))\n",
    "        if not crd or crd in excluded_ids:\n",
    "            continue\n",
    "        cmp_raw = _norm_str(row.get(\"compared_to\"))\n",
    "        if not cmp_raw:\n",
    "            continue\n",
    "\n",
    "        for nb in cmp_raw.split(\",\"):\n",
    "            nb = _norm_str(nb)\n",
    "            if not nb:\n",
    "                continue\n",
    "            if nb in excluded_ids:\n",
    "                # drop edges touching excluded nodes\n",
    "                continue\n",
    "            edges.append((crd, nb))\n",
    "            seen_nodes.add(crd)\n",
    "            seen_nodes.add(nb)\n",
    "\n",
    "    # Disjoint-set (Union-Find)\n",
    "    parent = {}\n",
    "\n",
    "    def find(x):\n",
    "        parent.setdefault(x, x)\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "\n",
    "    def union(a, b):\n",
    "        ra, rb = find(a), find(b)\n",
    "        if ra != rb:\n",
    "            parent[rb] = ra\n",
    "\n",
    "    for a, b in edges:\n",
    "        union(a, b)\n",
    "\n",
    "    for n in seen_nodes:\n",
    "        find(n)\n",
    "\n",
    "    comps = defaultdict(list)\n",
    "    for n in parent.keys():\n",
    "        comps[find(n)].append(n)\n",
    "\n",
    "    return [tuple(sorted(v)) for v in comps.values()]\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Pair validation rules  (GUARDS AGAINST STARS)\n",
    "# ------------------------------\n",
    "\n",
    "def validate_pair(group_df: pd.DataFrame, threshold: float) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Validate a 2-row group against the pair rules.\n",
    "    Returns a list of violation dicts (empty if valid).\n",
    "\n",
    "    New behavior:\n",
    "      - If any member is a star (z_flag_homogenized==6) OR tie_result==3,\n",
    "        this pair should not exist (stars are excluded from comparisons).\n",
    "        A violation 'PAIR_STAR_SHOULD_NOT_BE_COMPARED' is emitted and other\n",
    "        checks for this pair are skipped.\n",
    "      - Stars must carry tie_result==3; otherwise 'STAR_MUST_BE_3'.\n",
    "    \"\"\"\n",
    "    vios = []\n",
    "\n",
    "    a, b = group_df.iloc[0], group_df.iloc[1]\n",
    "\n",
    "    tr_a, tr_b = int(_to_num(a[\"tie_result\"])), int(_to_num(b[\"tie_result\"]))\n",
    "    zf_a = _to_num(a[\"z_flag_homogenized\"])\n",
    "    zf_b = _to_num(b[\"z_flag_homogenized\"])\n",
    "\n",
    "    # Star checks\n",
    "    a_is_star = (zf_a == 6) or (tr_a == 3)\n",
    "    b_is_star = (zf_b == 6) or (tr_b == 3)\n",
    "\n",
    "    if a_is_star and tr_a != 3:\n",
    "        vios.append({\n",
    "            \"rule\": \"STAR_MUST_BE_3\",\n",
    "            \"message\": \"Row with z_flag_homogenized==6 must have tie_result==3.\",\n",
    "            \"group_ids\": tuple(group_df[\"CRD_ID\"].tolist()),\n",
    "            \"rows\": group_df.copy(),\n",
    "        })\n",
    "    if b_is_star and tr_b != 3:\n",
    "        vios.append({\n",
    "            \"rule\": \"STAR_MUST_BE_3\",\n",
    "            \"message\": \"Row with z_flag_homogenized==6 must have tie_result==3.\",\n",
    "            \"group_ids\": tuple(group_df[\"CRD_ID\"].tolist()),\n",
    "            \"rows\": group_df.copy(),\n",
    "        })\n",
    "\n",
    "    if a_is_star or b_is_star:\n",
    "        vios.append({\n",
    "            \"rule\": \"PAIR_STAR_SHOULD_NOT_BE_COMPARED\",\n",
    "            \"message\": \"Stars must be excluded from comparisons; this pair should not exist.\",\n",
    "            \"group_ids\": tuple(group_df[\"CRD_ID\"].tolist()),\n",
    "            \"rows\": group_df.copy(),\n",
    "        })\n",
    "        return vios  # skip further pair logic\n",
    "\n",
    "    # --- original pair logic (unchanged) ---\n",
    "    # For comparisons, ignore flag 6 as \"quality\"\n",
    "    zf_a_eff = (-1 if zf_a == 6 else zf_a)\n",
    "    zf_b_eff = (-1 if zf_b == 6 else zf_b)\n",
    "\n",
    "    ts_a = _type_score(a[\"instrument_type_homogenized\"])\n",
    "    ts_b = _type_score(b[\"instrument_type_homogenized\"])\n",
    "\n",
    "    z_a = _to_num(a[\"z\"])\n",
    "    z_b = _to_num(b[\"z\"])\n",
    "    dz = _z_diff(z_a, z_b)  # np.inf if undefined\n",
    "\n",
    "    # Case: (1,0) or (0,1)\n",
    "    if {tr_a, tr_b} == {0, 1}:\n",
    "        win = a if tr_a == 1 else b\n",
    "        los = b if tr_a == 1 else a\n",
    "\n",
    "        win_zf = _to_num(win[\"z_flag_homogenized\"])\n",
    "        los_zf = _to_num(los[\"z_flag_homogenized\"])\n",
    "        win_zf_eff = (-1 if win_zf == 6 else win_zf)\n",
    "        los_zf_eff = (-1 if los_zf == 6 else los_zf)\n",
    "\n",
    "        win_ts = _type_score(win[\"instrument_type_homogenized\"])\n",
    "        los_ts = _type_score(los[\"instrument_type_homogenized\"])\n",
    "\n",
    "        win_z = _to_num(win[\"z\"])\n",
    "        los_z = _to_num(los[\"z\"])\n",
    "        win_los_dz = _z_diff(win_z, los_z)\n",
    "\n",
    "        cond = (\n",
    "            (win_zf_eff > los_zf_eff) or\n",
    "            (win_zf_eff == los_zf_eff and win_ts > los_ts) or\n",
    "            (win_zf_eff == los_zf_eff and win_ts == los_ts and win_los_dz < threshold)\n",
    "        )\n",
    "        if not cond:\n",
    "            vios.append({\n",
    "                \"rule\": \"PAIR_1v0_PRIORITY\",\n",
    "                \"message\": \"Winner does not have higher z_flag (excl. 6), nor better type, nor Œîz < threshold on tie.\",\n",
    "                \"group_ids\": tuple(group_df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": group_df.copy(),\n",
    "            })\n",
    "\n",
    "    # Case: (2,2)\n",
    "    elif tr_a == 2 and tr_b == 2:\n",
    "        cond_equal_quality = (zf_a_eff == zf_b_eff) and (ts_a == ts_b)\n",
    "        cond_delta = (dz > threshold) or (math.isinf(dz))\n",
    "        if not (cond_equal_quality and cond_delta):\n",
    "            vios.append({\n",
    "                \"rule\": \"PAIR_2v2_TIE_CONSISTENCY\",\n",
    "                \"message\": \"Tie (2,2) requires equal z_flag (excl. 6), equal type, and Œîz > threshold (or undefined).\",\n",
    "                \"group_ids\": tuple(group_df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": group_df.copy(),\n",
    "            })\n",
    "\n",
    "    # Case: (0,0)\n",
    "    elif tr_a == 0 and tr_b == 0:\n",
    "        # With star exclusion, (0,0) should be rare; keep original check\n",
    "        if not (zf_a == 6 and zf_b == 6):\n",
    "            vios.append({\n",
    "                \"rule\": \"PAIR_0v0_BOTH_FLAG6\",\n",
    "                \"message\": \"Both eliminated in a pair must have z_flag_homogenized == 6.\",\n",
    "                \"group_ids\": tuple(group_df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": group_df.copy(),\n",
    "            })\n",
    "\n",
    "    else:\n",
    "        vios.append({\n",
    "            \"rule\": \"PAIR_INVALID_TIE_PATTERN\",\n",
    "            \"message\": f\"Unexpected tie_result pattern for pair: ({tr_a},{tr_b}).\",\n",
    "            \"group_ids\": tuple(group_df[\"CRD_ID\"].tolist()),\n",
    "            \"rows\": group_df.copy(),\n",
    "        })\n",
    "\n",
    "    return vios\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Group validation rules (size >= 3)  (GUARDS AGAINST STARS)\n",
    "# ------------------------------\n",
    "\n",
    "def validate_group(group_df: pd.DataFrame, threshold: float) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Validate a group (>=3) with hierarchical constraints.\n",
    "\n",
    "    New behavior:\n",
    "      - If any member is a star (z_flag_homogenized==6 OR tie_result==3),\n",
    "        this group should not exist (stars are excluded from comparisons).\n",
    "        Emit 'GROUP_STAR_SHOULD_NOT_BE_COMPARED' and skip the rest.\n",
    "      - Stars must carry tie_result==3; otherwise 'STAR_MUST_BE_3'.\n",
    "    \"\"\"\n",
    "    vios = []\n",
    "    df = group_df.copy()\n",
    "\n",
    "    # Early star checks\n",
    "    zf = pd.to_numeric(df[\"z_flag_homogenized\"], errors=\"coerce\")\n",
    "    tr = pd.to_numeric(df[\"tie_result\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "    star_mask = (zf == 6) | (tr == 3)\n",
    "    if star_mask.any():\n",
    "        # Any star must have tie_result==3\n",
    "        bad_star = star_mask & (tr != 3)\n",
    "        if bad_star.any():\n",
    "            vios.append({\n",
    "                \"rule\": \"STAR_MUST_BE_3\",\n",
    "                \"message\": \"Rows with z_flag_homogenized==6 must have tie_result==3.\",\n",
    "                \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": df.copy(),\n",
    "            })\n",
    "        vios.append({\n",
    "            \"rule\": \"GROUP_STAR_SHOULD_NOT_BE_COMPARED\",\n",
    "            \"message\": \"Stars must be excluded from comparisons; this group should not exist.\",\n",
    "            \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "            \"rows\": df.copy(),\n",
    "        })\n",
    "        return vios  # skip further group logic\n",
    "\n",
    "    # --- original group logic (unchanged beyond star guard) ---\n",
    "    it = df[\"instrument_type_homogenized\"].map(_type_score).astype(\"Int64\")\n",
    "    z = pd.to_numeric(df[\"z\"], errors=\"coerce\")\n",
    "\n",
    "    # Survivors = rows with tie_result in {1,2}\n",
    "    survivors_mask = tr.isin({1, 2})\n",
    "    survivors_idx = df.index[survivors_mask]\n",
    "    losers_idx = df.index[~survivors_mask]\n",
    "\n",
    "    # 0) All-flag-6 special case: all must be 0 (shouldn't happen now)\n",
    "    if zf.notna().all() and (zf == 6).all():\n",
    "        if survivors_mask.any():\n",
    "            vios.append({\n",
    "                \"rule\": \"GROUP_ALL_FLAG6_ALL_ZERO\",\n",
    "                \"message\": \"All members have flag 6 but there are survivors.\",\n",
    "                \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": df.copy(),\n",
    "            })\n",
    "        return vios\n",
    "\n",
    "    # 1) Flag dominance (ignore 6 as quality)\n",
    "    zf_eff = zf.where(zf != 6, other=-1)\n",
    "    max_flag = zf_eff.max(skipna=True)\n",
    "    bad_flag_survivors = survivors_idx[zf_eff.loc[survivors_idx] < max_flag]\n",
    "    if len(bad_flag_survivors) > 0:\n",
    "        vios.append({\n",
    "            \"rule\": \"GROUP_FLAG_DOMINANCE\",\n",
    "            \"message\": \"Survivors include members with lower z_flag than group max (excluding 6).\",\n",
    "            \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "            \"rows\": df.copy(),\n",
    "        })\n",
    "\n",
    "    # 2) Type dominance among max-flag candidates\n",
    "    max_flag_mask = (zf_eff == max_flag)\n",
    "    if max_flag_mask.any():\n",
    "        max_type = it[max_flag_mask].max(skipna=True)\n",
    "        bad_type_survivors = survivors_idx[it.loc[survivors_idx] < max_type]\n",
    "        if len(bad_type_survivors) > 0:\n",
    "            vios.append({\n",
    "                \"rule\": \"GROUP_TYPE_DOMINANCE\",\n",
    "                \"message\": \"Survivors include members with lower instrument_type than group max.\",\n",
    "                \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": df.copy(),\n",
    "            })\n",
    "\n",
    "    # 3) Œîz-threshold independence among survivors at max-flag & max-type\n",
    "    cand_mask = (zf_eff == max_flag) & (it == it[max_flag_mask].max(skipna=True))\n",
    "    cand_idx = df.index[cand_mask]\n",
    "    crd = df.loc[cand_idx, \"CRD_ID\"].astype(str).tolist()\n",
    "    zvals = z.loc[cand_idx].to_numpy()\n",
    "\n",
    "    survivors_crd = set(df.loc[survivors_idx, \"CRD_ID\"].astype(str))\n",
    "\n",
    "    for i in range(len(crd)):\n",
    "        for j in range(i + 1, len(crd)):\n",
    "            zi, zj = zvals[i], zvals[j]\n",
    "            if not (np.isnan(zi) or np.isnan(zj)):\n",
    "                if abs(zi - zj) < threshold and (crd[i] in survivors_crd) and (crd[j] in survivors_crd):\n",
    "                    vios.append({\n",
    "                        \"rule\": \"GROUP_DELTZ_INDEPENDENCE\",\n",
    "                        \"message\": \"Two survivors are closer than threshold (both z defined). Survivors must form an independent set.\",\n",
    "                        \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "                        \"rows\": df.copy(),\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "    # 4) Final labeling consistency\n",
    "    n_survivors = int(survivors_mask.sum())\n",
    "    if n_survivors == 0:\n",
    "        if not (zf.notna().all() and (zf == 6).all()):\n",
    "            vios.append({\n",
    "                \"rule\": \"GROUP_NO_SURVIVOR_SUSPECT\",\n",
    "                \"message\": \"No survivors but group not all flag 6.\",\n",
    "                \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": df.copy(),\n",
    "            })\n",
    "    elif n_survivors == 1:\n",
    "        lone_idx = survivors_idx[0]\n",
    "        if int(df.loc[lone_idx, \"tie_result\"]) != 1:\n",
    "            vios.append({\n",
    "                \"rule\": \"GROUP_SINGLE_SURVIVOR_MUST_BE_1\",\n",
    "                \"message\": \"Exactly one survivor but not labeled with tie_result == 1.\",\n",
    "                \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": df.copy(),\n",
    "            })\n",
    "    else:\n",
    "        if not (df.loc[survivors_idx, \"tie_result\"] == 2).all():\n",
    "            vios.append({\n",
    "                \"rule\": \"GROUP_MULTI_SURVIVOR_MUST_BE_2\",\n",
    "                \"message\": \"Multiple survivors but not all labeled tie_result == 2.\",\n",
    "                \"group_ids\": tuple(df[\"CRD_ID\"].tolist()),\n",
    "                \"rows\": df.copy(),\n",
    "            })\n",
    "\n",
    "    return vios\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Public API  (excludes stars from graph + enforces tie_result==3)\n",
    "# ------------------------------\n",
    "\n",
    "def validate_tie_results(\n",
    "    df_final: pd.DataFrame,\n",
    "    threshold: float = 0.0005,\n",
    "    max_groups: int | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Main entry point.\n",
    "    - Excludes stars (z_flag_homogenized==6 or tie_result==3) from graph.\n",
    "    - Builds components from 'compared_to' among NON-stars.\n",
    "    - Validates pairs and groups with the declared rules.\n",
    "    - Validates that all stars have tie_result==3.\n",
    "    \"\"\"\n",
    "    df = df_final.copy()\n",
    "    df[\"CRD_ID\"] = df[\"CRD_ID\"].astype(str)\n",
    "    df[\"compared_to\"] = df[\"compared_to\"].astype(\"string\")\n",
    "\n",
    "    # Identify stars & enforce tie_result==3\n",
    "    zf = pd.to_numeric(df.get(\"z_flag_homogenized\"), errors=\"coerce\")\n",
    "    tr = pd.to_numeric(df.get(\"tie_result\"), errors=\"coerce\")\n",
    "    star_mask = (zf == 6) | (tr == 3)\n",
    "    star_ids = set(df.loc[star_mask, \"CRD_ID\"].astype(str))\n",
    "    # Pre-collect violations for stars with wrong tie_result\n",
    "    violations = []\n",
    "    wrong_tr_mask = (zf == 6) & (tr != 3)\n",
    "    if wrong_tr_mask.any():\n",
    "        rows = df.loc[wrong_tr_mask].copy()\n",
    "        violations.append({\n",
    "            \"rule\": \"STAR_MUST_BE_3\",\n",
    "            \"message\": \"Rows with z_flag_homogenized==6 must have tie_result==3.\",\n",
    "            \"group_ids\": tuple(rows[\"CRD_ID\"].tolist()),\n",
    "            \"rows\": rows,\n",
    "        })\n",
    "\n",
    "    # Build components EXCLUDING stars\n",
    "    components = build_components(df, excluded_ids=star_ids)\n",
    "    if max_groups is not None:\n",
    "        components = components[:max_groups]\n",
    "\n",
    "    n_pairs = n_groups = 0\n",
    "\n",
    "    for comp in components:\n",
    "        group_df = df[df[\"CRD_ID\"].isin(comp)].copy()\n",
    "        if len(group_df) == 2:\n",
    "            n_pairs += 1\n",
    "            violations.extend(validate_pair(group_df, threshold))\n",
    "        elif len(group_df) > 2:\n",
    "            n_groups += 1\n",
    "            violations.extend(validate_group(group_df, threshold))\n",
    "        # singleton components are irrelevant here\n",
    "\n",
    "    summary = {\n",
    "        \"n_components\": len(components),\n",
    "        \"n_pairs\": n_pairs,\n",
    "        \"n_groups\": n_groups,\n",
    "        \"n_violations\": len(violations),\n",
    "        \"by_rule\": pd.Series([v[\"rule\"] for v in violations]).value_counts().to_dict() if violations else {},\n",
    "        \"n_stars_excluded\": int(star_mask.sum()),\n",
    "    }\n",
    "    return {\"summary\": summary, \"violations\": violations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4322bed-d29d-4bad-b616-900bfc67479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = validate_tie_results(df_final, threshold=0.0005, max_groups=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8f208-bb9a-4a52-8988-e3a779032352",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report[\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e679b41-9cdd-4e0c-87a8-84b46ce89321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecione alguns casos\n",
    "viol = report[\"violations\"]\n",
    "for v in viol[:5]:\n",
    "    print(v[\"rule\"], \"->\", v[\"message\"], \"IDs:\", v[\"group_ids\"])\n",
    "    display(v[\"rows\"][[\n",
    "        \"CRD_ID\", \"ra\", \"dec\", \"z\",\"z_flag_homogenized\",\"instrument_type_homogenized\",\"tie_result\",\"survey\",\"source\",\"compared_to\"\n",
    "    ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385faf5b-843e-4413-863d-3c1df74f1a49",
   "metadata": {},
   "source": [
    "Checking if all values with compared_to <NA> have tie_result 1 (or 0, if z_flag_homogenized = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063868d-f77a-4b7a-9ee8-be76343d18e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validation: when compared_to is <NA>, tie_result must be 1\n",
    "# --- EXCEPTION: tie_result may be 3 ONLY if z_flag_homogenized == 6\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure compared_to is normalized to real NA (StringDtype shows <NA>)\n",
    "df_final_val = df_final.copy()\n",
    "df_final_val[\"compared_to\"] = (\n",
    "    df_final_val[\"compared_to\"]\n",
    "      .astype(\"string\").str.strip()\n",
    "      .replace({\"\": pd.NA, \"nan\": pd.NA, \"NaN\": pd.NA, \"NA\": pd.NA,\n",
    "                \"<NA>\": pd.NA, \"None\": pd.NA, \"null\": pd.NA})\n",
    ")\n",
    "\n",
    "# 1) Subset to rows where compared_to is truly missing\n",
    "na_cmp = df_final_val[df_final_val[\"compared_to\"].isna()].copy()\n",
    "\n",
    "# 2) Parse tie_result and z_flag_homogenized (para l√≥gica, mantendo -1)\n",
    "tie_as_int   = pd.to_numeric(na_cmp[\"tie_result\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "zflag_as_int = pd.to_numeric(na_cmp[\"z_flag_homogenized\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "\n",
    "# 3) Valid if:\n",
    "#    - tie_result == 1\n",
    "#    - OR tie_result == 3 and z_flag_homogenized == 6\n",
    "valid_mask = (tie_as_int.eq(1)) | (tie_as_int.eq(3) & zflag_as_int.eq(6))\n",
    "violations = na_cmp[~valid_mask].copy()\n",
    "\n",
    "# 4) Summary\n",
    "total_na      = len(na_cmp)\n",
    "valid_count   = int(valid_mask.sum())\n",
    "invalid_count = len(violations)\n",
    "\n",
    "print(\"===== compared_to <NA> validation (tie_result rule with flag-6 exception) =====\")\n",
    "print(f\"Rows with compared_to <NA>: {total_na}\")\n",
    "print(f\"Valid:                      {valid_count}\")\n",
    "print(f\"INVALID:                    {invalid_count}\")\n",
    "\n",
    "# 4b) Exibi√ß√£o sem -1: converte para Int64 -> string -> substitui NA\n",
    "tie_disp   = pd.to_numeric(na_cmp[\"tie_result\"], errors=\"coerce\").astype(\"Int64\").astype(\"string\").fillna(\"<NA>\")\n",
    "zflag_disp = pd.to_numeric(na_cmp[\"z_flag_homogenized\"], errors=\"coerce\").astype(\"Int64\").astype(\"string\").fillna(\"<NA>\")\n",
    "\n",
    "print(\"\\nCrosstab of tie_result x z_flag_homogenized (for display, <NA> shown):\")\n",
    "print(pd.crosstab(tie_disp, zflag_disp, dropna=False))\n",
    "\n",
    "# 5) Show some offending rows (if any)\n",
    "if invalid_count > 0:\n",
    "    cols_to_show = [\n",
    "        \"CRD_ID\", \"ra\", \"dec\", \"z\", \"z_flag\", \"z_err\",\n",
    "        \"z_flag_homogenized\", \"instrument_type\", \"instrument_type_homogenized\",\n",
    "        \"tie_result\", \"survey\", \"source\", \"compared_to\"\n",
    "    ]\n",
    "    print(\"\\n‚ö†Ô∏è Examples of violations (up to 10):\")\n",
    "    display(violations[cols_to_show].head(10))\n",
    "else:\n",
    "    print(\"\\n‚úÖ All rows with compared_to <NA> satisfy the rule (1, or 3 with flag==6).\")\n",
    "\n",
    "# 6) Optional hard assertion\n",
    "# assert invalid_count == 0, \"Found rows where compared_to is <NA> but tie_result is invalid (not 1, nor 3 with flag==6).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d37c4-fc0b-4600-9cb9-535f8b024d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleciona as linhas com z_flag_homogenized nulo (<NA>)\n",
    "na_zflag = df_final[df_final[\"z_flag_homogenized\"].isna()].copy()\n",
    "\n",
    "print(f\"Total de linhas com z_flag_homogenized <NA>: {len(na_zflag)}\")\n",
    "\n",
    "cols_to_show = [\n",
    "    \"CRD_ID\", \"ra\", \"dec\", \"z\", \"z_flag\", \"z_err\",\n",
    "    \"z_flag_homogenized\", \"instrument_type\", \"instrument_type_homogenized\",\n",
    "    \"tie_result\", \"survey\", \"source\", \"compared_to\"\n",
    "]\n",
    "\n",
    "# Mostra algumas linhas\n",
    "display(na_zflag[cols_to_show].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0576eb0-1af9-4965-bb4d-eeac56f0e456",
   "metadata": {},
   "source": [
    "Checking if some z_flag_homogenized 6 entered the comparisson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb2847-5cc2-4f76-9ff6-4be4a1c91dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Normalize compared_to so blanks/strings like \"nan\" become NA ---\n",
    "df_chk = df_final.copy()\n",
    "df_chk[\"compared_to_norm\"] = (\n",
    "    df_chk[\"compared_to\"]\n",
    "      .astype(\"string\").str.strip()\n",
    "      .replace({\"\": pd.NA, \"nan\": pd.NA, \"NaN\": pd.NA, \"NA\": pd.NA,\n",
    "                \"<NA>\": pd.NA, \"None\": pd.NA, \"null\": pd.NA})\n",
    ")\n",
    "\n",
    "# --- Check condition only on rows with z_flag_homogenized == 6 ---\n",
    "mask6 = df_chk[\"z_flag_homogenized\"].eq(6)\n",
    "all_na = df_chk.loc[mask6, \"compared_to_norm\"].isna().all()\n",
    "\n",
    "print(f\"Todos os z_flag_homogenized==6 t√™m compared_to <NA>? {all_na}\")\n",
    "\n",
    "# Optional: list offending rows if any\n",
    "if not all_na:\n",
    "    offenders = df_chk.loc[\n",
    "        mask6 & df_chk[\"compared_to_norm\"].notna(),\n",
    "        [\"CRD_ID\", \"z\", \"z_flag_homogenized\", \"survey\", \"source\", \"compared_to\", \"tie_result\"]\n",
    "    ].head(20)\n",
    "    display(offenders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f35e40-d969-4e3c-b91c-31bd567318ce",
   "metadata": {},
   "source": [
    "## Manual Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72123e-ecb3-47b4-bac3-9e131207f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_manual_comparisson = False\n",
    "\n",
    "if combine_mode != \"concatenate\" and do_manual_comparisson:\n",
    "    # =========================================================\n",
    "    # ANALYZE GROUPS BY COMPARED_TO\n",
    "    # =========================================================\n",
    "    import numpy as np\n",
    "    from collections import defaultdict, deque\n",
    "\n",
    "    threshold = 0.0005\n",
    "    max_groups = 10000  # For debugging or limiting processed groups\n",
    "\n",
    "    desired_order = [\n",
    "        \"CRD_ID\", \"id\", \"ra\", \"dec\", \"z\", \"z_flag\", \"z_err\", \"instrument_type\", \"survey\", \"source\",\n",
    "        \"z_flag_homogenized\", \"instrument_type_homogenized\", \"tie_result\", \"compared_to\", \"role\"\n",
    "    ]\n",
    "\n",
    "    # Filter valid rows: only rows with non-empty 'compared_to'\n",
    "    df_final_just_compared = df_final[\n",
    "        (df_final[\"compared_to\"].notnull()) &\n",
    "        (df_final[\"compared_to\"] != \"\")\n",
    "    ]\n",
    "    print(f\"‚úÖ Number of rows with non-empty compared_to: {len(df_final_just_compared)}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Build an undirected adjacency graph from 'compared_to' relationships\n",
    "    # ------------------------------------------------------------------\n",
    "    adjacency = defaultdict(set)\n",
    "    for _, row in df_final_just_compared.iterrows():\n",
    "        crd_id = row[\"CRD_ID\"]\n",
    "        for neighbor in str(row[\"compared_to\"]).split(\",\"):\n",
    "            nb = neighbor.strip()\n",
    "            if not nb:\n",
    "                continue\n",
    "            adjacency[crd_id].add(nb)\n",
    "            adjacency[nb].add(crd_id)\n",
    "\n",
    "    # BFS to get connected component starting at 'start_id'\n",
    "    def get_connected_group(start_id):\n",
    "        visited = set()\n",
    "        queue = deque([start_id])\n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            queue.extend(adjacency[current] - visited)\n",
    "        return tuple(sorted(visited))\n",
    "\n",
    "    # Buckets for case studies / inspections\n",
    "    group_cases = {\n",
    "        \"CASE1_small_same\": [],\n",
    "        \"CASE1_small_diff\": [],\n",
    "        \"CASE1_large_same\": [],\n",
    "        \"CASE1_large_diff\": [],\n",
    "        \"CASE2_small_same\": [],\n",
    "        \"CASE2_small_diff\": [],\n",
    "        \"CASE2_large_same\": [],\n",
    "        \"CASE2_large_diff\": [],\n",
    "        \"TIE_FLAG_TYPE_BREAK_PAIR\": [],\n",
    "        \"TIE_FLAG_TYPE_BREAK_GROUP\": [],\n",
    "        \"SAME_FLAG_DIFF_TYPE\": [],\n",
    "        \"SAME_SOURCE_PAIR\": []  # NEW: pairs with identical `source`\n",
    "    }\n",
    "\n",
    "    processed_groups = 0\n",
    "    seen_groups = set()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Loop over rows; for each connected group, compute diagnostics once\n",
    "    # ------------------------------------------------------------------\n",
    "    for idx, row in df_final_just_compared.iterrows():\n",
    "        if max_groups is not None and processed_groups >= max_groups:\n",
    "            break\n",
    "\n",
    "        group_ids = get_connected_group(row[\"CRD_ID\"])\n",
    "        if group_ids in seen_groups:\n",
    "            continue\n",
    "        seen_groups.add(group_ids)\n",
    "\n",
    "        group_df = df_final_just_compared[df_final_just_compared[\"CRD_ID\"].isin(group_ids)].copy()\n",
    "        if len(group_df) < 2:\n",
    "            continue\n",
    "\n",
    "        # Mark the row used to discover the group (only for display)\n",
    "        group_df[\"role\"] = np.where(group_df[\"CRD_ID\"] == row[\"CRD_ID\"], \"principal\", \"compared\")\n",
    "\n",
    "        # Pairwise Œîz and survey signature\n",
    "        z_vals = group_df[\"z\"].to_numpy()\n",
    "        surveys = group_df[\"survey\"].values\n",
    "\n",
    "        delta_z_matrix = np.abs(z_vals[:, None] - z_vals[None, :])\n",
    "        pairwise_dz = delta_z_matrix[np.triu_indices(len(z_vals), k=1)]\n",
    "\n",
    "        max_delta_z = float(np.max(pairwise_dz)) if len(pairwise_dz) else 0.0\n",
    "        all_pairs_below_thresh = bool(np.all(pairwise_dz <= threshold)) if len(pairwise_dz) else True\n",
    "        same_survey = len(set(surveys)) == 1\n",
    "\n",
    "        # Classification by size (pair/group), Œîz, and survey consistency\n",
    "        if len(group_df) == 2:\n",
    "            key = (\n",
    "                \"CASE1_small_same\" if (max_delta_z <= threshold and same_survey) else\n",
    "                \"CASE1_small_diff\" if (max_delta_z <= threshold) else\n",
    "                \"CASE1_large_same\" if (same_survey) else\n",
    "                \"CASE1_large_diff\"\n",
    "            )\n",
    "        else:\n",
    "            key = (\n",
    "                \"CASE2_small_same\" if (all_pairs_below_thresh and same_survey) else\n",
    "                \"CASE2_small_diff\" if (all_pairs_below_thresh) else\n",
    "                \"CASE2_large_same\" if (same_survey) else\n",
    "                \"CASE2_large_diff\"\n",
    "            )\n",
    "\n",
    "        # Reorder columns for readability\n",
    "        all_columns = list(group_df.columns)\n",
    "        ordered_columns = desired_order + [col for col in all_columns if col not in desired_order]\n",
    "        group_df = group_df.reindex(columns=ordered_columns)\n",
    "\n",
    "        group_cases[key].append(group_df)\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Additional tie-breaking investigations / buckets\n",
    "        # -----------------------------------------------\n",
    "        flags = set(group_df[\"z_flag_homogenized\"].dropna())\n",
    "        types = set(group_df[\"instrument_type_homogenized\"].dropna())\n",
    "        surveys_in_group = set(group_df[\"survey\"].dropna())\n",
    "\n",
    "        # Case: same flag, different type, different surveys ‚Äî split by size\n",
    "        if len(surveys_in_group) > 1 and len(flags) == 1 and len(types) > 1:\n",
    "            if len(group_df) == 2:\n",
    "                group_cases[\"TIE_FLAG_TYPE_BREAK_PAIR\"].append(group_df)\n",
    "            elif len(group_df) > 2:\n",
    "                group_cases[\"TIE_FLAG_TYPE_BREAK_GROUP\"].append(group_df)\n",
    "\n",
    "        # Case: same flag, different types ‚Äî only for true groups (>2)\n",
    "        if len(flags) == 1 and len(types) > 1 and len(group_df) > 2:\n",
    "            group_cases[\"SAME_FLAG_DIFF_TYPE\"].append(group_df)\n",
    "\n",
    "        # NEW: Case ‚Äî pairs with the same `source` (normalized, non-null, exact match)\n",
    "        if len(group_df) == 2:\n",
    "            src_series = group_df[\"source\"]\n",
    "            src_valid = src_series.dropna().astype(str).str.strip().str.lower()\n",
    "            if len(src_valid) == 2 and len(set(src_valid)) == 1:\n",
    "                group_cases[\"SAME_SOURCE_PAIR\"].append(group_df)\n",
    "\n",
    "        processed_groups += 1\n",
    "\n",
    "    print(f\"‚úÖ Processed {processed_groups} unique groups.\")\n",
    "\n",
    "    # Human-readable descriptions for each bucket\n",
    "    case_descriptions = {\n",
    "        \"CASE1_small_same\": f\"pair with delta_z <= {threshold} from same survey\",\n",
    "        \"CASE1_small_diff\": f\"pair with delta_z <= {threshold} from different surveys\",\n",
    "        \"CASE1_large_same\": f\"pair with delta_z > {threshold} from same survey\",\n",
    "        \"CASE1_large_diff\": f\"pair with delta_z > {threshold} from different surveys\",\n",
    "        \"CASE2_small_same\": f\"group with all delta_z <= {threshold} from same survey\",\n",
    "        \"CASE2_small_diff\": f\"group with all delta_z <= {threshold} from different surveys\",\n",
    "        \"CASE2_large_same\": f\"group with some delta_z > {threshold} from same survey\",\n",
    "        \"CASE2_large_diff\": f\"group with some delta_z > {threshold} from different surveys\",\n",
    "        \"TIE_FLAG_TYPE_BREAK_PAIR\": \"pair with equal z_flag_homogenized, different instrument_type_homogenized, and different surveys\",\n",
    "        \"TIE_FLAG_TYPE_BREAK_GROUP\": \"group with equal z_flag_homogenized, different instrument_type_homogenized, and different surveys\",\n",
    "        \"SAME_FLAG_DIFF_TYPE\": \"group with same z_flag_homogenized but at least one differing instrument_type_homogenized\",\n",
    "        \"SAME_SOURCE_PAIR\": \"pair with identical source (normalized, non-null)\"  # NEW\n",
    "    }\n",
    "\n",
    "    # Final columns to display for examples\n",
    "    final_columns = [\n",
    "        \"CRD_ID\", \"ra\", \"dec\", \"z\", \"z_flag\", \"z_err\",\n",
    "        \"z_flag_homogenized\", \"instrument_type\", \"instrument_type_homogenized\",\n",
    "        \"tie_result\", \"survey\", \"source\", \"compared_to\"\n",
    "    ]\n",
    "\n",
    "    def survey_signature(df):\n",
    "        \"\"\"\n",
    "        Signature = sorted tuple of the group's surveys (for diversity sampling).\n",
    "        Example: ('VANDELS', 'VVDS')\n",
    "        \"\"\"\n",
    "        vals = df[\"survey\"].dropna().astype(str).unique().tolist()\n",
    "        return tuple(sorted(vals)) if len(vals) else (\"<MISSING>\",)\n",
    "\n",
    "    MAX_EXAMPLES_PER_CASE = 5  # show up to 5 groups per bucket\n",
    "\n",
    "    for case_name, groups in group_cases.items():\n",
    "        if not groups:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüìå Showing examples of {case_descriptions[case_name]} \"\n",
    "              f\"({len(groups)} groups found):\")\n",
    "\n",
    "        # 1) Prefer groups with unique survey signatures to increase diversity\n",
    "        seen_sigs = set()\n",
    "        diverse_selection = []\n",
    "        leftovers = []\n",
    "\n",
    "        for g in groups:\n",
    "            sig = survey_signature(g)\n",
    "            if sig not in seen_sigs:\n",
    "                seen_sigs.add(sig)\n",
    "                diverse_selection.append(g)\n",
    "            else:\n",
    "                leftovers.append(g)\n",
    "\n",
    "            if len(diverse_selection) >= MAX_EXAMPLES_PER_CASE:\n",
    "                break\n",
    "\n",
    "        # 2) If needed, fill remaining slots with leftover groups (allow repetition)\n",
    "        i = 0\n",
    "        while len(diverse_selection) < MAX_EXAMPLES_PER_CASE and i < len(leftovers):\n",
    "            diverse_selection.append(leftovers[i])\n",
    "            i += 1\n",
    "\n",
    "        # 3) Display in order with the final selected columns\n",
    "        for group in diverse_selection:\n",
    "            group_to_show = group.reindex(columns=final_columns)\n",
    "            display(group_to_show)\n",
    "            print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"If you want to do the manual comparisson, change the variable in the top of this cell to True. Be aware that this can take a long time.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801271a1-eeb4-402d-a97c-46de5cb47376",
   "metadata": {},
   "source": [
    "## Validation - Prepared Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c38638-040b-43cc-9c0b-fb22ba0e0dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dicion√°rio de regras equivalente ao YAML\n",
    "translation_rules = {\n",
    "    \"2DFGRS\": {\n",
    "        \"z_flag_translation\": {1: 0, 2: 1, 3: 3, 4: 4, 5: 4},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"2DFLENS\": {\n",
    "        \"z_flag_translation\": {1: 0, 2: 1, 3: 3, 4: 4, 6: 6},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"2MRS\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"z_err == 0\", \"value\": 3},\n",
    "                {\"expr\": \"0 < z_err < 0.0005\", \"value\": 4},\n",
    "                {\"expr\": \"z_err >= 0.0005\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"3D-HST\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"z_best_s == 0\", \"value\": 6},\n",
    "                {\"expr\": \"z_best_s == 1 and z_spec != -1\", \"value\": 4},\n",
    "                {\"expr\": \"z_best_s == 2 and use_zgrism == 1 and flag1 == 0 and flag2 == 0\", \"value\": 3},\n",
    "                {\"expr\": \"z_best_s == 3 and use_phot == 1\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"z_best_s == 1\", \"value\": \"s\"},\n",
    "                {\"expr\": \"z_best_s == 2\", \"value\": \"g\"},\n",
    "                {\"expr\": \"z_best_s == 3\", \"value\": \"p\"},\n",
    "            ],\n",
    "            \"default\": \"g\",\n",
    "        },\n",
    "    },\n",
    "    \"6DFGS\": {\n",
    "        \"z_flag_translation\": {1: 0, 2: 1, 3: 3, 4: 4, 6: 6},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"ASTRODEEP\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"zspec_survey != '-'\", \"value\": 4},\n",
    "                {\"expr\": \"zspec_survey == '-'\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"zspec_survey != '-'\", \"value\": \"s\"},\n",
    "                {\"expr\": \"zspec_survey == '-'\", \"value\": \"p\"},\n",
    "            ],\n",
    "            \"default\": \"p\",\n",
    "        },\n",
    "    },\n",
    "    \"ASTRODEEP-JWST\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"zspec != -99 and z_flag < 400 and (len(str(int(z_flag))) <= 1 or int(str(int(z_flag))[-2]) <= 3)\", \"value\": 4},\n",
    "                {\"expr\": \"zspec == -99 and z_flag < 400 and (len(str(int(z_flag))) <= 1 or int(str(int(z_flag))[-2]) <= 3)\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"zspec != -99\", \"value\": \"s\"},\n",
    "                {\"expr\": \"zspec == -99\", \"value\": \"p\"},\n",
    "            ],\n",
    "            \"default\": \"p\",\n",
    "        },\n",
    "    },\n",
    "    \"DESI\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"ZCAT_PRIMARY != True\", \"value\": 0},\n",
    "                {\"expr\": \"z_flag != 0 and ZCAT_PRIMARY == True\", \"value\": 1},\n",
    "                {\"expr\": \"z_flag == 0 and ZCAT_PRIMARY == True and z_err < 0.0005\", \"value\": 4},\n",
    "                {\"expr\": \"z_flag == 0 and ZCAT_PRIMARY == True and z_err >= 0.0005\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"JADES\": {\n",
    "        \"z_flag_translation\": {4: 4, 3: 3, 2: 2, 1: 1, 0: 0},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"MOSDEF\": {\n",
    "        \"z_flag_translation\": {7: 4, 6: 3, 5: 2, 4: 2, 3: 1, 2: 1, 1: 0, 0: 0},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"OZDES\": {\n",
    "        \"z_flag_translation\": {1: 0, 2: 1, 3: 3, 4: 4, 6: 6},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"PRIMUS\": {\n",
    "        \"z_flag_translation\": {-1: 0, 2: 1, 3: 2, 4: 3},\n",
    "        \"instrument_type_translation\": {\"default\": \"g\"},\n",
    "    },\n",
    "    \"VANDELS\": {\n",
    "        \"z_flag_translation\": {\n",
    "            0: 0, 1: 1, 2: 2, 3: 4, 4: 4, 9: 3,\n",
    "            10: 0, 11: 1, 12: 2, 13: 4, 14: 4, 19: 3,\n",
    "            20: 0, 21: 1, 22: 2, 23: 4, 24: 4, 29: 3,\n",
    "            210: 0, 211: 1, 212: 2, 213: 4, 214: 4, 219: 3,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"VIMOS\": {\n",
    "        \"z_flag_translation\": {4: 4, 3: 3, 2: 2, 1: 1, 0: 0},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"VUDS\": {\n",
    "        \"z_flag_translation\": {\n",
    "            1: 1, 11: 1, 21: 1, 31: 1, 41: 1,\n",
    "            2: 2, 12: 2, 22: 2, 32: 2, 42: 2, 9: 2, 19: 2, 29: 2, 39: 2, 49: 2,\n",
    "            3: 3, 13: 3, 23: 3, 33: 3, 43: 3,\n",
    "            4: 4, 14: 4, 24: 4, 34: 4, 44: 4,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"VVDS\": {\n",
    "        \"z_flag_translation\": {\n",
    "            0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 9: 2,\n",
    "            10: 0, 11: 1, 12: 2, 13: 3, 14: 4, 19: 2,\n",
    "            20: 0, 21: 1, 22: 2, 23: 3, 24: 4, 29: 2,\n",
    "            210: 0, 211: 1, 212: 2, 213: 3, 214: 4, 219: 2,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "\n",
    "    # Special cases using continuous rule and inherited type\n",
    "    \"CANDELS\": {\"_special\": \"CANDELS_NED\"},\n",
    "    \"NED\": {\"_special\": \"CANDELS_NED\"},\n",
    "}\n",
    "\n",
    "def _safe_eval_expr(expr: str, ctx: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Avalia 'expr' usando apenas vari√°veis do ctx e fun√ß√µes b√°sicas.\n",
    "    Retorna True/False; se der erro, retorna False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Permitir apenas fun√ß√µes b√°sicas e numpy\n",
    "        allowed_globals = {\n",
    "            \"__builtins__\": {\"len\": len, \"int\": int, \"str\": str, \"float\": float},\n",
    "            \"np\": np,\n",
    "        }\n",
    "        return bool(eval(expr, allowed_globals, ctx))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _apply_translation(value_map, row_ctx):\n",
    "    \"\"\"\n",
    "    value_map pode ser:\n",
    "      - dict simples {orig: dest} (pode conter 'default')\n",
    "      - dict com 'conditions' (lista de {expr, value}) e opcional 'default'\n",
    "    Retorna (valor_traduzido, matched_bool)\n",
    "    \"\"\"\n",
    "    if isinstance(value_map, dict) and \"conditions\" in value_map:\n",
    "        for cond in value_map[\"conditions\"]:\n",
    "            expr = cond.get(\"expr\", \"\")\n",
    "            val = cond.get(\"value\", np.nan)\n",
    "            if expr and _safe_eval_expr(expr, row_ctx):\n",
    "                return val, True\n",
    "        # nenhum matched -> usa default se houver\n",
    "        if \"default\" in value_map:\n",
    "            return value_map[\"default\"], True\n",
    "        return np.nan, False\n",
    "\n",
    "    # mapeamento direto (sem 'conditions'):\n",
    "    if isinstance(value_map, dict):\n",
    "        key = row_ctx.get(\"z_flag\", np.nan)\n",
    "        if key in value_map:\n",
    "            return value_map[key], True\n",
    "        # Se n√£o houver chave correspondente, mas existir 'default', use-o\n",
    "        if \"default\" in value_map:\n",
    "            return value_map[\"default\"], True\n",
    "        return np.nan, False\n",
    "\n",
    "    return np.nan, False\n",
    "\n",
    "def validate_row(row):\n",
    "    survey = row.get(\"survey\", None)\n",
    "\n",
    "    # construir contexto com None -> np.nan, para evitar erros de compara√ß√£o\n",
    "    ctx = {}\n",
    "    for k, v in row.items():\n",
    "        ctx[k] = (np.nan if v is None else v)\n",
    "\n",
    "    # Casos especiais (CANDELS e NED): regra cont√≠nua 0..1 e type herdado\n",
    "    if survey in (\"CANDELS\", \"NED\"):\n",
    "        x = row.get(\"z_flag\", np.nan)\n",
    "        # z_flag esperado:\n",
    "        if x == 0.0:\n",
    "            z_expected = 0.0\n",
    "        elif (isinstance(x, (float, int))) and (0.0 < x < 0.7):\n",
    "            z_expected = 1.0\n",
    "        elif (isinstance(x, (float, int))) and (0.7 <= x < 0.9):\n",
    "            z_expected = 2.0\n",
    "        elif (isinstance(x, (float, int))) and (0.9 <= x < 0.99):\n",
    "            z_expected = 3.0\n",
    "        elif (isinstance(x, (float, int))) and (0.99 <= x <= 1.0):\n",
    "            z_expected = 4.0\n",
    "        else:\n",
    "            z_expected = np.nan\n",
    "\n",
    "        # type_expected √© o pr√≥prio 'type' da linha\n",
    "        type_expected = row.get(\"instrument_type\", np.nan)\n",
    "        return z_expected, type_expected\n",
    "\n",
    "    # Regras gerais dos surveys\n",
    "    rules = translation_rules.get(survey, None)\n",
    "    if rules is None:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # z_flag_homogenized esperado\n",
    "    z_rules = rules.get(\"z_flag_translation\", None)\n",
    "    if z_rules is None:\n",
    "        z_expected = np.nan\n",
    "    else:\n",
    "        z_expected, _ = _apply_translation(z_rules, ctx)\n",
    "\n",
    "    # instrument_type_homogenized esperado\n",
    "    t_rules = rules.get(\"instrument_type_translation\", None)\n",
    "    if t_rules is None:\n",
    "        type_expected = np.nan\n",
    "    else:\n",
    "        if isinstance(t_rules, dict) and (\"conditions\" in t_rules or \"default\" in t_rules):\n",
    "            type_expected, matched = _apply_translation(t_rules, ctx)\n",
    "        else:\n",
    "            type_expected, matched = _apply_translation(t_rules, ctx)\n",
    "\n",
    "\n",
    "    return z_expected, type_expected\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# VALIDATE TRANSLATIONS IN TEMP FILES\n",
    "# =========================================================\n",
    "merged_files = glob.glob(os.path.join(prepared_temp_dir, \"prepared*/*.parquet\"))\n",
    "merged_files = [f for f in merged_files if \"pipeline_sample\" not in f]\n",
    "\n",
    "if not merged_files:\n",
    "    print(\"‚ö†Ô∏è No prepared parquet files found for validation.\")\n",
    "else:\n",
    "    issues = []\n",
    "    \n",
    "    for merged_file in merged_files:\n",
    "        print(f\"üîç Validating {merged_file}\")\n",
    "        df = pd.read_parquet(merged_file)\n",
    "    \n",
    "        for _, row in df.iterrows():\n",
    "            z_exp, type_exp = validate_row(row)\n",
    "    \n",
    "            if not (pd.isna(z_exp) and pd.isna(row[\"z_flag_homogenized\"])) and z_exp != row[\"z_flag_homogenized\"]:\n",
    "                issue = row.to_dict()\n",
    "                issue[\"field\"] = \"z_flag_homogenized\"\n",
    "                issue[\"expected\"] = z_exp\n",
    "                issue[\"found\"] = row[\"z_flag_homogenized\"]\n",
    "                issues.append(issue)\n",
    "    \n",
    "            if not (pd.isna(type_exp) and pd.isna(row[\"instrument_type_homogenized\"])) and type_exp != row[\"instrument_type_homogenized\"]:\n",
    "                issue = row.to_dict()\n",
    "                issue[\"field\"] = \"instrument_type_homogenized\"\n",
    "                issue[\"expected\"] = type_exp\n",
    "                issue[\"found\"] = row[\"instrument_type_homogenized\"]\n",
    "                issues.append(issue)\n",
    "    \n",
    "    if issues:\n",
    "        issues_df = pd.DataFrame(issues)\n",
    "        display(issues_df)\n",
    "        print(f\"‚ö†Ô∏è {len(issues)} mismatches found!\")\n",
    "    else:\n",
    "        print(\"‚úÖ All homogenized fields match the expected values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0adcb-e34d-4e81-90e1-8079773eedb6",
   "metadata": {},
   "source": [
    "# Time Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417dc417-cee6-45ba-8306-952ac9bf03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================================\n",
    "# 1. CONFIGURA√á√ÉO\n",
    "# ============================================\n",
    "\n",
    "log_dir = \"process001/process_info\"\n",
    "\n",
    "log_files = [\n",
    "    \"prepare_all.log\",\n",
    "    \"import_all.log\",\n",
    "    \"margin_cache_all.log\",\n",
    "    \"crossmatch_and_merge_all.log\",\n",
    "    \"process.log\"\n",
    "]\n",
    "\n",
    "START_RE = re.compile(\n",
    "    r\"(?P<timestamp>\\d{4}-\\d{2}-\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d+): Starting: (?P<task>[\\w_]+) id=(?P<id>[\\w\\d_]+)\"\n",
    ")\n",
    "FINISH_RE = re.compile(\n",
    "    r\"(?P<timestamp>\\d{4}-\\d{2}-\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d+): Finished: (?P<task>[\\w_]+) id=(?P<id>[\\w\\d_]+)\"\n",
    ")\n",
    "\n",
    "# Vamos manter SEMPRE o primeiro start e o √∫ltimo finish de cada task|id\n",
    "start_times = {}\n",
    "end_times = {}\n",
    "\n",
    "# ============================================\n",
    "# 2. LEITURA E PARSE DOS LOGS\n",
    "# ============================================\n",
    "\n",
    "for file in log_files:\n",
    "    path = os.path.join(log_dir, file)\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            m_start = START_RE.search(line)\n",
    "            m_finish = FINISH_RE.search(line)\n",
    "\n",
    "            if m_start:\n",
    "                task_id = f\"{m_start.group('task')}|{m_start.group('id')}\"\n",
    "                ts = datetime.strptime(m_start.group(\"timestamp\"), \"%Y-%m-%d-%H:%M:%S.%f\")\n",
    "                # guarda o menor (primeiro) start\n",
    "                if task_id not in start_times:\n",
    "                    start_times[task_id] = ts\n",
    "                else:\n",
    "                    if ts < start_times[task_id]:\n",
    "                        start_times[task_id] = ts\n",
    "\n",
    "            if m_finish:\n",
    "                task_id = f\"{m_finish.group('task')}|{m_finish.group('id')}\"\n",
    "                ts = datetime.strptime(m_finish.group(\"timestamp\"), \"%Y-%m-%d-%H:%M:%S.%f\")\n",
    "                # guarda o maior (√∫ltimo) finish\n",
    "                if task_id not in end_times:\n",
    "                    end_times[task_id] = ts\n",
    "                else:\n",
    "                    if ts > end_times[task_id]:\n",
    "                        end_times[task_id] = ts\n",
    "\n",
    "# ============================================\n",
    "# 3. CONSTRU√á√ÉO DO EIXO Y EM ORDEM CUSTOMIZADA\n",
    "# ============================================\n",
    "\n",
    "# Considere apenas tasks que t√™m start e end\n",
    "all_ids = sorted(set(start_times) & set(end_times))\n",
    "\n",
    "pipeline_init_id = \"pipeline_init|pipeline_init\"\n",
    "consolidate_id = \"consolidate|consolidate\"\n",
    "\n",
    "# IDs de prepare_catalog individuais (exclui o agregador prepare_catalogs)\n",
    "prepare_ids = [\n",
    "    tid for tid in all_ids\n",
    "    if tid.startswith(\"prepare_catalog|\") and tid != \"prepare_catalogs|prepare_catalogs\"\n",
    "]\n",
    "\n",
    "# Ordene os prepares pelo start: quem come√ßa antes aparece antes -> y menor -> \"mais abaixo\" visualmente\n",
    "prepare_ids_sorted = sorted(prepare_ids, key=lambda tid: start_times[tid])\n",
    "\n",
    "# Import cat0 inicial, se existir\n",
    "import_cat0 = [tid for tid in all_ids if tid == \"import_catalog|cat0_hats\"]\n",
    "\n",
    "# Demais tasks (exceto pipeline_init, consolidate, prepares e import_cat0)\n",
    "remaining_ids = [\n",
    "    tid for tid in all_ids\n",
    "    if tid not in prepare_ids + import_cat0 + [pipeline_init_id, consolidate_id]\n",
    "]\n",
    "\n",
    "# Agrupar por step num√©rico (catX, merged_stepX, etc.) para ordenar dentro dos steps\n",
    "step_dict = defaultdict(list)\n",
    "for tid in remaining_ids:\n",
    "    match = re.search(r\"(?:cat|merged_step)(\\d+)\", tid)\n",
    "    if match:\n",
    "        step = int(match.group(1))\n",
    "        step_dict[step].append(tid)\n",
    "\n",
    "ordered_step_ids = []\n",
    "for step in sorted(step_dict):\n",
    "    step_tasks = step_dict[step]\n",
    "\n",
    "    def task_order(tid):\n",
    "        if tid.startswith(\"import_catalog|cat\"):\n",
    "            return 0\n",
    "        elif tid.startswith(\"generate_margin_cache\"):\n",
    "            return 1\n",
    "        elif tid.startswith(\"crossmatch_and_merge\"):\n",
    "            return 2\n",
    "        elif tid.startswith(\"import_catalog|merged_step\"):\n",
    "            return 3\n",
    "        else:\n",
    "            return 99\n",
    "\n",
    "    # Dentro do step, mant√©m uma ordem l√≥gica pelas \"fases\"\n",
    "    ordered_step_ids.extend(sorted(step_tasks, key=task_order))\n",
    "\n",
    "# Monta a ordem final: pipeline_init -> prepares (ordenados pelo start) -> import_cat0 -> steps -> consolidate\n",
    "ordered_ids = []\n",
    "if pipeline_init_id in all_ids:\n",
    "    ordered_ids.append(pipeline_init_id)\n",
    "ordered_ids.extend(prepare_ids_sorted)\n",
    "ordered_ids.extend(import_cat0)\n",
    "ordered_ids.extend(ordered_step_ids)\n",
    "if consolidate_id in all_ids:\n",
    "    ordered_ids.append(consolidate_id)\n",
    "\n",
    "# ============================================\n",
    "# 4. MONTAGEM DOS DADOS PARA O PLOT\n",
    "# ============================================\n",
    "\n",
    "# Tempo adicional a ser subtra√≠do do in√≠cio do pipeline_init (em segundos)\n",
    "aditional_pipeline_init_time = 3  # ‚è±Ô∏è ajuste aqui conforme necess√°rio\n",
    "\n",
    "# Se existir pipeline_init, ajusta o in√≠cio para \"andar\" um pouco antes (apenas para est√©tica)\n",
    "if pipeline_init_id in start_times:\n",
    "    start_times[pipeline_init_id] -= timedelta(seconds=aditional_pipeline_init_time)\n",
    "\n",
    "# Zera o tempo no primeiro start dentre os que vamos plotar\n",
    "start_zero = min(start_times[tid] for tid in ordered_ids)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# üõ†Ô∏è INSERIR REGISTRO MANUAL DA TAREFA \"register\"\n",
    "# --------------------------------------------------\n",
    "register_id = \"register|register\"\n",
    "register_duration = 3  # ‚è±Ô∏è dura√ß√£o da tarefa \"register\" em segundos\n",
    "\n",
    "# Insere o \"register\" ao final\n",
    "ordered_ids.append(register_id)\n",
    "# Come√ßa ap√≥s o consolidate (se existir) ou ap√≥s o √∫ltimo t√©rmino conhecido\n",
    "if consolidate_id in end_times:\n",
    "    register_start = max(end_times[consolidate_id], *end_times.values())\n",
    "else:\n",
    "    register_start = max(end_times.values())\n",
    "register_end = register_start + timedelta(seconds=register_duration)\n",
    "\n",
    "start_times[register_id] = register_start\n",
    "end_times[register_id] = register_end\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Constr√≥i listas relativas ao start_zero\n",
    "y_labels = []\n",
    "start_list = []\n",
    "end_list = []\n",
    "\n",
    "for tid in ordered_ids:\n",
    "    y_labels.append(tid)\n",
    "    start_rel = (start_times[tid] - start_zero).total_seconds()\n",
    "    end_rel = (end_times[tid] - start_zero).total_seconds()\n",
    "    start_list.append(start_rel)\n",
    "    end_list.append(end_rel)\n",
    "\n",
    "# ============================================\n",
    "# 4b. AJUSTAR POSI√á√ïES Y PARA SEPARAR \"register\"\n",
    "# ============================================\n",
    "\n",
    "# Cria posi√ß√µes Y padr√£o e separa o √∫ltimo (register) com um espa√ßamento extra\n",
    "y_positions = list(range(len(ordered_ids)))\n",
    "y_positions[-1] += 5.0  # üõ†Ô∏è Aumenta a posi√ß√£o do \"register\" no eixo Y\n",
    "\n",
    "# ============================================\n",
    "# 5. PLOTAGEM DO GR√ÅFICO DE TIME PROFILE\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# === Mapear cores por grupo\n",
    "group_colors = {\n",
    "    \"pipeline_init\": \"#003f5c\",       # azul escuro\n",
    "    \"prepare_catalogs\": \"#b8860b\",    # amarelo escuro\n",
    "    \"crossmatch\": \"#2f855a\",          # verde escuro\n",
    "    \"consolidate\": \"#003f5c\",         # mesmo do pipeline_init\n",
    "    \"register\": \"#003f5c\",            # mesmo azul escuro do pipeline_init\n",
    "}\n",
    "\n",
    "# === Determinar grupo de cada tarefa\n",
    "def get_group(tid):\n",
    "    if tid == pipeline_init_id:\n",
    "        return \"pipeline_init\"\n",
    "    elif tid == register_id:\n",
    "        return \"register\"\n",
    "    elif tid in prepare_ids:\n",
    "        return \"prepare_catalogs\"\n",
    "    elif tid == consolidate_id:\n",
    "        return \"consolidate\"\n",
    "    else:\n",
    "        return \"crossmatch\"\n",
    "\n",
    "# === Plotar tarefas com cor unificada para linha e bolinhas\n",
    "for y, start, end, tid in zip(y_positions, start_list, end_list, ordered_ids):\n",
    "    group = get_group(tid)\n",
    "    color = group_colors[group]\n",
    "    plt.hlines(y, start, end, colors=color, linewidth=2)\n",
    "    plt.scatter(start, y, color=color, s=10)  # in√≠cio\n",
    "    plt.scatter(end, y, color=color, s=10)    # fim\n",
    "\n",
    "# ============================================\n",
    "# Agrupar labels do eixo Y por grupo\n",
    "# ============================================\n",
    "\n",
    "group_positions = defaultdict(list)\n",
    "for y, tid in zip(y_positions, ordered_ids):\n",
    "    group_positions[get_group(tid)].append(y)\n",
    "\n",
    "group_labels = []\n",
    "group_ticks = []\n",
    "\n",
    "for label in [\"pipeline_init\", \"prepare_catalogs\", \"crossmatch\", \"consolidate\", \"register\"]:\n",
    "    if group_positions[label]:\n",
    "        center = sum(group_positions[label]) / len(group_positions[label])\n",
    "        group_labels.append(label)\n",
    "        group_ticks.append(center)\n",
    "\n",
    "# ============================================\n",
    "# Personaliza√ß√£o final do gr√°fico\n",
    "# ============================================\n",
    "\n",
    "plt.yticks(group_ticks, group_labels, fontsize=20)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Time (s)\", fontsize=20)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_preparation",
   "language": "python",
   "name": "data_preparation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
