{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d2c6aa-e836-4c37-b67f-b8a828c80590",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b9448-1cdf-46a1-b07c-b031c3c5f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "user = getpass.getuser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921fa07e-c9e5-4ff5-bfa9-ff27115717d6",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66b946-64d0-4182-aca2-1268f6b3a2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# CONFIGURATION SECTION\n",
    "# =========================================================\n",
    "#input_paths = [\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/2dfgrs_final_release.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/2dflens_final_release.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/2mrs_v240.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/3dhst_v4.1.5.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/6dfgs_dr3.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/astrodeep_jwst.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/astrodeep-gs43.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/desi_dr1_in_lsst_dp1_fields.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/jades_dr3.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/mosdef_final_release.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/ozdes_dr2.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/primus_dr1.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/vandels_dr4.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/vlt_vimos_v2.0.1.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/processed/vuds_dr1.parquet\",\n",
    "#    f\"/scratch/users/{user}/speczs-catalogs/processed/vvds_final_release.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/johns-catalogs/z_cat_CANDELS_clean_sitcomtn-154.parquet\",\n",
    "#    f\"/scratch/users/luigi.silva/speczs-catalogs/johns-catalogs/z_cat_NED_clean_sitcomtn-154.parquet\",\n",
    "#]\n",
    "\n",
    "input_paths = glob.glob('test_data/*.parquet')\n",
    "\n",
    "# replace paths in case of using local env \n",
    "final_catalog_path = f\"./process001/outputs/crd.parquet\"\n",
    "prepared_temp_dir = f\"./process001/temp/\"\n",
    "\n",
    "combine_mode = \"concatenate_and_mark_duplicates\" # Options: \"concatenate\", \"concatenate_and_mark_duplicates\", or \"concatenate_and_remove_duplicates\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b1390e-4d78-46dc-a879-93741200e42d",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9d742f-2815-4c78-b471-ed538be842b7",
   "metadata": {},
   "source": [
    "## Validation - Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2c2af-86a5-4c2c-abfb-3f304ed3b90d",
   "metadata": {},
   "source": [
    "Counting input and output rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0ff59-ef63-4ff9-886f-37606a01cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# COUNT INPUT ROWS\n",
    "# =========================================================\n",
    "total_input_rows = 0\n",
    "for path in input_paths:\n",
    "    if os.path.exists(path):\n",
    "        parquet_file = pq.ParquetFile(path)\n",
    "        n_rows = parquet_file.metadata.num_rows\n",
    "        print(f\"{path} -> {n_rows} rows\")\n",
    "        total_input_rows += n_rows\n",
    "    else:\n",
    "        warnings.warn(f\"‚ö†Ô∏è File not found: {path}\")\n",
    "\n",
    "print(f\"‚úÖ Total number of input rows: {total_input_rows}\")\n",
    "\n",
    "# =========================================================\n",
    "# LOAD FINAL MERGED CATALOG\n",
    "# =========================================================\n",
    "if not os.path.exists(final_catalog_path):\n",
    "    raise FileNotFoundError(f\"‚ùå Final catalog not found: {final_catalog_path}\")\n",
    "\n",
    "df_final = pd.read_parquet(final_catalog_path)\n",
    "print(f\"‚úÖ Total number of rows in final catalog: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0918b-8f97-44f5-bf8c-2cb41ba2b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a85f2-2deb-419b-84f0-893ccbf4ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9d0d0-c23e-41d4-a01c-3e53161f01eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db02aa8-5dea-41c3-9807-a18308f4b93e",
   "metadata": {},
   "source": [
    "Basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff02fa-50be-4d05-ae3b-d62ae76df68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f2bdc-55a9-463c-859b-9dbb1ecd1e57",
   "metadata": {},
   "source": [
    "Counting tie_result values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf326961-1cf0-4587-a47c-bbec4a99dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if combine_mode != \"concatenate\":\n",
    "    print(df_final[\"tie_result\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd6ce4-6d28-484f-bf65-68b44085a852",
   "metadata": {},
   "source": [
    "Counting survey values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a2a90c-386c-4904-824a-547509576407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e8dd0-f6cc-4310-a896-fba216515967",
   "metadata": {},
   "source": [
    "Checking the percentage of unsolved objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d2df1-3df5-45a3-a1bb-217a2925c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "if combine_mode != \"concatenate\":\n",
    "    # Total number of objects\n",
    "    total_all = len(df_final)\n",
    "    \n",
    "    # Filter objects that were compared (compared_to is not null or empty)\n",
    "    mask_compared = df_final[\"compared_to\"].notna() & (df_final[\"compared_to\"] != \"\")\n",
    "    df_compared = df_final[mask_compared]\n",
    "    \n",
    "    # Count how many have tie_result == 2\n",
    "    count_tie2 = (df_final[\"tie_result\"] == 2).sum()\n",
    "    count_tie2_compared = (df_compared[\"tie_result\"] == 2).sum()\n",
    "    \n",
    "    # Percentages\n",
    "    percent_all = (count_tie2 / total_all) * 100 if total_all > 0 else 0\n",
    "    percent_compared = (count_tie2_compared / len(df_compared)) * 100 if len(df_compared) > 0 else 0\n",
    "    \n",
    "    # Formatted print\n",
    "    print(f\"üìä tie_result == 2 represents:\")\n",
    "    print(f\"  ‚Ä¢ {percent_all:.2f}% of the total ({count_tie2} out of {total_all})\")\n",
    "    print(f\"  ‚Ä¢ {percent_compared:.2f}% of the compared objects ({count_tie2_compared} out of {len(df_compared)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d3621-1cf4-4aa1-94f8-6a6f7dfb5985",
   "metadata": {},
   "source": [
    "Doing the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e2afd-890c-466a-a119-442d9230e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "if combine_mode != \"concatenate\":\n",
    "    # =========================================================\n",
    "    # ANALYZE GROUPS BY COMPARED_TO\n",
    "    # =========================================================\n",
    "    import numpy as np\n",
    "    from collections import defaultdict, deque\n",
    "\n",
    "    threshold = 0.0005\n",
    "    max_groups = 10000  # For debugging or limiting processed groups\n",
    "\n",
    "    desired_order = [\n",
    "        \"CRD_ID\", \"id\", \"ra\", \"dec\", \"z\", \"z_flag\", \"z_err\", \"instrument_type\", \"survey\", \"source\",\n",
    "        \"z_flag_homogenized\", \"instrument_type_homogenized\", \"tie_result\", \"compared_to\", \"role\"\n",
    "    ]\n",
    "\n",
    "    # Filter valid rows: only rows with non-empty 'compared_to'\n",
    "    df_final_just_compared = df_final[\n",
    "        (df_final[\"compared_to\"].notnull()) &\n",
    "        (df_final[\"compared_to\"] != \"\")\n",
    "    ]\n",
    "    print(f\"‚úÖ Number of rows with non-empty compared_to: {len(df_final_just_compared)}\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Build an undirected adjacency graph from 'compared_to' relationships\n",
    "    # ------------------------------------------------------------------\n",
    "    adjacency = defaultdict(set)\n",
    "    for _, row in df_final_just_compared.iterrows():\n",
    "        crd_id = row[\"CRD_ID\"]\n",
    "        for neighbor in str(row[\"compared_to\"]).split(\",\"):\n",
    "            nb = neighbor.strip()\n",
    "            if not nb:\n",
    "                continue\n",
    "            adjacency[crd_id].add(nb)\n",
    "            adjacency[nb].add(crd_id)\n",
    "\n",
    "    # BFS to get connected component starting at 'start_id'\n",
    "    def get_connected_group(start_id):\n",
    "        visited = set()\n",
    "        queue = deque([start_id])\n",
    "        while queue:\n",
    "            current = queue.popleft()\n",
    "            if current in visited:\n",
    "                continue\n",
    "            visited.add(current)\n",
    "            queue.extend(adjacency[current] - visited)\n",
    "        return tuple(sorted(visited))\n",
    "\n",
    "    # Buckets for case studies / inspections\n",
    "    group_cases = {\n",
    "        \"CASE1_small_same\": [],\n",
    "        \"CASE1_small_diff\": [],\n",
    "        \"CASE1_large_same\": [],\n",
    "        \"CASE1_large_diff\": [],\n",
    "        \"CASE2_small_same\": [],\n",
    "        \"CASE2_small_diff\": [],\n",
    "        \"CASE2_large_same\": [],\n",
    "        \"CASE2_large_diff\": [],\n",
    "        \"TIE_FLAG_TYPE_BREAK_PAIR\": [],\n",
    "        \"TIE_FLAG_TYPE_BREAK_GROUP\": [],\n",
    "        \"SAME_FLAG_DIFF_TYPE\": [],\n",
    "        \"SAME_SOURCE_PAIR\": []  # NEW: pairs with identical `source`\n",
    "    }\n",
    "\n",
    "    processed_groups = 0\n",
    "    seen_groups = set()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Loop over rows; for each connected group, compute diagnostics once\n",
    "    # ------------------------------------------------------------------\n",
    "    for idx, row in df_final_just_compared.iterrows():\n",
    "        if max_groups is not None and processed_groups >= max_groups:\n",
    "            break\n",
    "\n",
    "        group_ids = get_connected_group(row[\"CRD_ID\"])\n",
    "        if group_ids in seen_groups:\n",
    "            continue\n",
    "        seen_groups.add(group_ids)\n",
    "\n",
    "        group_df = df_final_just_compared[df_final_just_compared[\"CRD_ID\"].isin(group_ids)].copy()\n",
    "        if len(group_df) < 2:\n",
    "            continue\n",
    "\n",
    "        # Mark the row used to discover the group (only for display)\n",
    "        group_df[\"role\"] = np.where(group_df[\"CRD_ID\"] == row[\"CRD_ID\"], \"principal\", \"compared\")\n",
    "\n",
    "        # Pairwise Œîz and survey signature\n",
    "        z_vals = group_df[\"z\"].to_numpy()\n",
    "        surveys = group_df[\"survey\"].values\n",
    "\n",
    "        delta_z_matrix = np.abs(z_vals[:, None] - z_vals[None, :])\n",
    "        pairwise_dz = delta_z_matrix[np.triu_indices(len(z_vals), k=1)]\n",
    "\n",
    "        max_delta_z = float(np.max(pairwise_dz)) if len(pairwise_dz) else 0.0\n",
    "        all_pairs_below_thresh = bool(np.all(pairwise_dz <= threshold)) if len(pairwise_dz) else True\n",
    "        same_survey = len(set(surveys)) == 1\n",
    "\n",
    "        # Classification by size (pair/group), Œîz, and survey consistency\n",
    "        if len(group_df) == 2:\n",
    "            key = (\n",
    "                \"CASE1_small_same\" if (max_delta_z <= threshold and same_survey) else\n",
    "                \"CASE1_small_diff\" if (max_delta_z <= threshold) else\n",
    "                \"CASE1_large_same\" if (same_survey) else\n",
    "                \"CASE1_large_diff\"\n",
    "            )\n",
    "        else:\n",
    "            key = (\n",
    "                \"CASE2_small_same\" if (all_pairs_below_thresh and same_survey) else\n",
    "                \"CASE2_small_diff\" if (all_pairs_below_thresh) else\n",
    "                \"CASE2_large_same\" if (same_survey) else\n",
    "                \"CASE2_large_diff\"\n",
    "            )\n",
    "\n",
    "        # Reorder columns for readability\n",
    "        all_columns = list(group_df.columns)\n",
    "        ordered_columns = desired_order + [col for col in all_columns if col not in desired_order]\n",
    "        group_df = group_df.reindex(columns=ordered_columns)\n",
    "\n",
    "        group_cases[key].append(group_df)\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Additional tie-breaking investigations / buckets\n",
    "        # -----------------------------------------------\n",
    "        flags = set(group_df[\"z_flag_homogenized\"].dropna())\n",
    "        types = set(group_df[\"instrument_type_homogenized\"].dropna())\n",
    "        surveys_in_group = set(group_df[\"survey\"].dropna())\n",
    "\n",
    "        # Case: same flag, different type, different surveys ‚Äî split by size\n",
    "        if len(surveys_in_group) > 1 and len(flags) == 1 and len(types) > 1:\n",
    "            if len(group_df) == 2:\n",
    "                group_cases[\"TIE_FLAG_TYPE_BREAK_PAIR\"].append(group_df)\n",
    "            elif len(group_df) > 2:\n",
    "                group_cases[\"TIE_FLAG_TYPE_BREAK_GROUP\"].append(group_df)\n",
    "\n",
    "        # Case: same flag, different types ‚Äî only for true groups (>2)\n",
    "        if len(flags) == 1 and len(types) > 1 and len(group_df) > 2:\n",
    "            group_cases[\"SAME_FLAG_DIFF_TYPE\"].append(group_df)\n",
    "\n",
    "        # NEW: Case ‚Äî pairs with the same `source` (normalized, non-null, exact match)\n",
    "        if len(group_df) == 2:\n",
    "            src_series = group_df[\"source\"]\n",
    "            src_valid = src_series.dropna().astype(str).str.strip().str.lower()\n",
    "            if len(src_valid) == 2 and len(set(src_valid)) == 1:\n",
    "                group_cases[\"SAME_SOURCE_PAIR\"].append(group_df)\n",
    "\n",
    "        processed_groups += 1\n",
    "\n",
    "    print(f\"‚úÖ Processed {processed_groups} unique groups.\")\n",
    "\n",
    "    # Human-readable descriptions for each bucket\n",
    "    case_descriptions = {\n",
    "        \"CASE1_small_same\": f\"pair with delta_z <= {threshold} from same survey\",\n",
    "        \"CASE1_small_diff\": f\"pair with delta_z <= {threshold} from different surveys\",\n",
    "        \"CASE1_large_same\": f\"pair with delta_z > {threshold} from same survey\",\n",
    "        \"CASE1_large_diff\": f\"pair with delta_z > {threshold} from different surveys\",\n",
    "        \"CASE2_small_same\": f\"group with all delta_z <= {threshold} from same survey\",\n",
    "        \"CASE2_small_diff\": f\"group with all delta_z <= {threshold} from different surveys\",\n",
    "        \"CASE2_large_same\": f\"group with some delta_z > {threshold} from same survey\",\n",
    "        \"CASE2_large_diff\": f\"group with some delta_z > {threshold} from different surveys\",\n",
    "        \"TIE_FLAG_TYPE_BREAK_PAIR\": \"pair with equal z_flag_homogenized, different instrument_type_homogenized, and different surveys\",\n",
    "        \"TIE_FLAG_TYPE_BREAK_GROUP\": \"group with equal z_flag_homogenized, different instrument_type_homogenized, and different surveys\",\n",
    "        \"SAME_FLAG_DIFF_TYPE\": \"group with same z_flag_homogenized but at least one differing instrument_type_homogenized\",\n",
    "        \"SAME_SOURCE_PAIR\": \"pair with identical source (normalized, non-null)\"  # NEW\n",
    "    }\n",
    "\n",
    "    # Final columns to display for examples\n",
    "    final_columns = [\n",
    "        \"CRD_ID\", \"ra\", \"dec\", \"z\", \"z_flag\", \"z_err\",\n",
    "        \"z_flag_homogenized\", \"instrument_type\", \"instrument_type_homogenized\",\n",
    "        \"tie_result\", \"survey\", \"source\", \"compared_to\"\n",
    "    ]\n",
    "\n",
    "    def survey_signature(df):\n",
    "        \"\"\"\n",
    "        Signature = sorted tuple of the group's surveys (for diversity sampling).\n",
    "        Example: ('VANDELS', 'VVDS')\n",
    "        \"\"\"\n",
    "        vals = df[\"survey\"].dropna().astype(str).unique().tolist()\n",
    "        return tuple(sorted(vals)) if len(vals) else (\"<MISSING>\",)\n",
    "\n",
    "    MAX_EXAMPLES_PER_CASE = 5  # show up to 5 groups per bucket\n",
    "\n",
    "    for case_name, groups in group_cases.items():\n",
    "        if not groups:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüìå Showing examples of {case_descriptions[case_name]} \"\n",
    "              f\"({len(groups)} groups found):\")\n",
    "\n",
    "        # 1) Prefer groups with unique survey signatures to increase diversity\n",
    "        seen_sigs = set()\n",
    "        diverse_selection = []\n",
    "        leftovers = []\n",
    "\n",
    "        for g in groups:\n",
    "            sig = survey_signature(g)\n",
    "            if sig not in seen_sigs:\n",
    "                seen_sigs.add(sig)\n",
    "                diverse_selection.append(g)\n",
    "            else:\n",
    "                leftovers.append(g)\n",
    "\n",
    "            if len(diverse_selection) >= MAX_EXAMPLES_PER_CASE:\n",
    "                break\n",
    "\n",
    "        # 2) If needed, fill remaining slots with leftover groups (allow repetition)\n",
    "        i = 0\n",
    "        while len(diverse_selection) < MAX_EXAMPLES_PER_CASE and i < len(leftovers):\n",
    "            diverse_selection.append(leftovers[i])\n",
    "            i += 1\n",
    "\n",
    "        # 3) Display in order with the final selected columns\n",
    "        for group in diverse_selection:\n",
    "            group_to_show = group.reindex(columns=final_columns)\n",
    "            display(group_to_show)\n",
    "            print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee8798-249c-485b-a681-cf388960f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validate tie_result patterns for ALL connected groups built from `compared_to` ---\n",
    "# Rules:\n",
    "# - Pair (size == 2): allowed patterns are (2,2), (1,0), and (0,0) ONLY if both z_flag_homogenized == 6\n",
    "# - Group (size > 2): allowed patterns are:\n",
    "#     * exactly one \"1\" and the rest \"0\" (single winner)\n",
    "#     * mix of some \"2\" and some \"0\" (no \"1\"s)\n",
    "#     * all \"2\"\n",
    "#     * possibly all \"0\" ONLY if all have z_flag_homogenized == 6\n",
    "#\n",
    "# We will:\n",
    "# 1) build connected groups via `compared_to`\n",
    "# 2) classify each group into one of the categories below (including INVALID buckets)\n",
    "# 3) print a few examples per category (if any)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque, Counter\n",
    "from IPython.display import display\n",
    "\n",
    "# ------------ Helpers ------------\n",
    "def _parse_neighbors(s):\n",
    "    \"\"\"Parse a comma-separated 'compared_to' field into a clean list of IDs as strings.\"\"\"\n",
    "    if pd.isna(s) or not str(s).strip():\n",
    "        return []\n",
    "    return [x.strip() for x in str(s).split(\",\") if x.strip()]\n",
    "\n",
    "def _build_adjacency(df):\n",
    "    \"\"\"Build an undirected adjacency mapping from CRD_ID <-> compared_to.\"\"\"\n",
    "    adj = defaultdict(set)\n",
    "    for _, r in df.iterrows():\n",
    "        u = r[\"CRD_ID\"]\n",
    "        for v in _parse_neighbors(r[\"compared_to\"]):\n",
    "            # Keep IDs as strings for consistency; also add reciprocal link\n",
    "            u_str = str(u)\n",
    "            v_str = str(v)\n",
    "            adj[u_str].add(v_str)\n",
    "            adj[v_str].add(u_str)\n",
    "    return adj\n",
    "\n",
    "def _get_connected(start, adj, seen):\n",
    "    \"\"\"BFS over adjacency; returns sorted tuple of nodes in the component.\"\"\"\n",
    "    q = deque([start])\n",
    "    comp = []\n",
    "    while q:\n",
    "        cur = q.popleft()\n",
    "        if cur in seen:\n",
    "            continue\n",
    "        seen.add(cur)\n",
    "        comp.append(cur)\n",
    "        q.extend(adj[cur] - set(comp) - seen)\n",
    "    return tuple(sorted(comp))\n",
    "\n",
    "def _all_flag6(group):\n",
    "    \"\"\"True if all z_flag_homogenized == 6 (ignoring NaNs means NOT all 6).\"\"\"\n",
    "    zf = pd.to_numeric(group[\"z_flag_homogenized\"], errors=\"coerce\")\n",
    "    return zf.notna().all() and (zf == 6).all()\n",
    "\n",
    "def _as_int_series(s):\n",
    "    \"\"\"Coerce tie_result to integers; NaNs become -1 (invalid).\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\").fillna(-1).astype(int)\n",
    "\n",
    "# Columns to display in examples\n",
    "final_columns = [\n",
    "    \"CRD_ID\", \"ra\", \"dec\", \"z\", \"z_flag\", \"z_err\",\n",
    "    \"z_flag_homogenized\", \"instrument_type\", \"instrument_type_homogenized\",\n",
    "    \"tie_result\", \"survey\", \"source\", \"compared_to\"\n",
    "]\n",
    "\n",
    "# ------------ Filter rows with usable compared_to ------------\n",
    "_work = df_final.copy()\n",
    "_work = _work[(_work[\"compared_to\"].notna()) & (_work[\"compared_to\"].astype(str).str.strip() != \"\")]\n",
    "print(f\"‚úÖ Rows with non-empty compared_to: {len(_work)}\")\n",
    "\n",
    "# ------------ Build groups ------------\n",
    "adjacency = _build_adjacency(_work)\n",
    "seen = set()\n",
    "groups = []\n",
    "for node in adjacency.keys():\n",
    "    if node not in seen:\n",
    "        comp = _get_connected(node, adjacency, seen)\n",
    "        if len(comp) >= 1:\n",
    "            groups.append(comp)\n",
    "\n",
    "print(f\"‚úÖ Connected groups found (considering only rows with non-empty compared_to): {len(groups)}\")\n",
    "\n",
    "# ------------ Classify groups by tie_result pattern ------------\n",
    "CATS = {\n",
    "    # Pairs\n",
    "    \"PAIR_22\": [],\n",
    "    \"PAIR_10\": [],\n",
    "    \"PAIR_00_ALL_FLAG6\": [],\n",
    "    \"PAIR_00_NOT_ALL_FLAG6_INVALID\": [],\n",
    "    \"PAIR_OTHER_INVALID\": [],\n",
    "    # Groups (>2)\n",
    "    \"GROUP_SINGLE_WINNER_1and0\": [],\n",
    "    \"GROUP_MIX_2and0\": [],\n",
    "    \"GROUP_ALL_2\": [],\n",
    "    \"GROUP_ALL_0_ALL_FLAG6\": [],\n",
    "    \"GROUP_ALL_0_NOT_ALL_FLAG6_INVALID\": [],\n",
    "    \"GROUP_OTHER_INVALID\": [],\n",
    "}\n",
    "\n",
    "# Which categories are \"allowed\" (for summary)\n",
    "ALLOWED = {\n",
    "    \"PAIR_22\",\n",
    "    \"PAIR_10\",\n",
    "    \"PAIR_00_ALL_FLAG6\",\n",
    "    \"GROUP_SINGLE_WINNER_1and0\",\n",
    "    \"GROUP_MIX_2and0\",\n",
    "    \"GROUP_ALL_2\",\n",
    "    \"GROUP_ALL_0_ALL_FLAG6\",\n",
    "}\n",
    "\n",
    "for comp in groups:\n",
    "    # Note: CRD_ID in df is numeric or string; our adjacency keys are strings.\n",
    "    # We match on string-form to be robust, then bring back to the DataFrame subset.\n",
    "    comp_set_str = set(comp)\n",
    "    g = _work[_work[\"CRD_ID\"].astype(str).isin(comp_set_str)].copy()\n",
    "    if len(g) < 2:\n",
    "        continue  # ignore singletons for this validation\n",
    "\n",
    "    ties = _as_int_series(g[\"tie_result\"])\n",
    "    cnt = Counter(ties.tolist())\n",
    "    size = len(g)\n",
    "\n",
    "    if size == 2:\n",
    "        # Normalize as a sorted pair\n",
    "        pattern = tuple(sorted(ties.tolist()))\n",
    "        if pattern == (2, 2):\n",
    "            CATS[\"PAIR_22\"].append(g[final_columns])\n",
    "        elif pattern == (0, 1):\n",
    "            CATS[\"PAIR_10\"].append(g[final_columns])\n",
    "        elif pattern == (0, 0):\n",
    "            if _all_flag6(g):\n",
    "                CATS[\"PAIR_00_ALL_FLAG6\"].append(g[final_columns])\n",
    "            else:\n",
    "                CATS[\"PAIR_00_NOT_ALL_FLAG6_INVALID\"].append(g[final_columns])\n",
    "        else:\n",
    "            CATS[\"PAIR_OTHER_INVALID\"].append(g[final_columns])\n",
    "\n",
    "    else:\n",
    "        # Groups (>2)\n",
    "        n0 = cnt.get(0, 0)\n",
    "        n1 = cnt.get(1, 0)\n",
    "        n2 = cnt.get(2, 0)\n",
    "        total = sum(cnt.values())\n",
    "\n",
    "        if n1 == 1 and (n0 == total - 1) and n2 == 0:\n",
    "            # one winner (1) + the rest losers (0), no 2s\n",
    "            CATS[\"GROUP_SINGLE_WINNER_1and0\"].append(g[final_columns])\n",
    "        elif n2 > 0 and n1 == 0 and (n0 + n2 == total):\n",
    "            # some 2s + some 0s (no 1s)\n",
    "            if n0 == 0:\n",
    "                CATS[\"GROUP_ALL_2\"].append(g[final_columns])\n",
    "            else:\n",
    "                CATS[\"GROUP_MIX_2and0\"].append(g[final_columns])\n",
    "        elif n0 == total:\n",
    "            if _all_flag6(g):\n",
    "                CATS[\"GROUP_ALL_0_ALL_FLAG6\"].append(g[final_columns])\n",
    "            else:\n",
    "                CATS[\"GROUP_ALL_0_NOT_ALL_FLAG6_INVALID\"].append(g[final_columns])\n",
    "        else:\n",
    "            # Anything else (e.g., 1 mixed with 2; multiple winners; unexpected values)\n",
    "            CATS[\"GROUP_OTHER_INVALID\"].append(g[final_columns])\n",
    "\n",
    "# ------------ Summary ------------\n",
    "allowed_count = sum(len(CATS[k]) for k in ALLOWED)\n",
    "invalid_count = sum(len(CATS[k]) for k in CATS.keys() - ALLOWED)\n",
    "print(\"\\n===== Validation summary =====\")\n",
    "for k, v in CATS.items():\n",
    "    tag = \"‚úÖ\" if k in ALLOWED else \"‚ö†Ô∏è\"\n",
    "    print(f\"{tag} {k}: {len(v)} groups\")\n",
    "print(f\"TOTAL allowed groups: {allowed_count}\")\n",
    "print(f\"TOTAL invalid groups: {invalid_count}\")\n",
    "\n",
    "# ------------ Show a few examples per category ------------\n",
    "MAX_EXAMPLES_PER_CAT = 8  # tweak as you like\n",
    "\n",
    "for cat, lst in CATS.items():\n",
    "    if not lst:\n",
    "        continue\n",
    "    print(\"\\n\" + (\"=\" * 88))\n",
    "    print(f\"{'ALLOWED' if cat in ALLOWED else 'INVALID'} ‚Üí {cat}  |  examples: {min(MAX_EXAMPLES_PER_CAT, len(lst))} of {len(lst)}\")\n",
    "    print(\"=\" * 88)\n",
    "    for i, df_ex in enumerate(lst[:MAX_EXAMPLES_PER_CAT], start=1):\n",
    "        print(f\"\\n[{cat}] Example {i}\")\n",
    "        # Helpful quick glance at the pattern\n",
    "        tie_list = df_ex['tie_result'].tolist()\n",
    "        flag_list = df_ex['z_flag_homogenized'].tolist()\n",
    "        print(f\"tie_result: {tie_list} | z_flag_homogenized: {flag_list}\")\n",
    "        display(df_ex)\n",
    "        print(\"-\" * 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8cdcf-13e5-4b43-acd1-86b68d9f0190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validation: when compared_to is <NA>, tie_result must be 1\n",
    "# --- EXCEPTION: tie_result may be 0 ONLY if z_flag_homogenized == 6\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure compared_to is normalized to real NA (StringDtype shows <NA>)\n",
    "df_final_val = df_final.copy()\n",
    "df_final_val[\"compared_to\"] = (\n",
    "    df_final_val[\"compared_to\"]\n",
    "      .astype(\"string\").str.strip()\n",
    "      .replace({\"\": pd.NA, \"nan\": pd.NA, \"NaN\": pd.NA, \"NA\": pd.NA, \"<NA>\": pd.NA, \"None\": pd.NA, \"null\": pd.NA})\n",
    ")\n",
    "\n",
    "# 1) Subset to rows where compared_to is truly missing\n",
    "na_cmp = df_final_val[df_final_val[\"compared_to\"].isna()].copy()\n",
    "\n",
    "# 2) Parse tie_result and z_flag_homogenized\n",
    "tie_as_int   = pd.to_numeric(na_cmp[\"tie_result\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "zflag_as_int = pd.to_numeric(na_cmp[\"z_flag_homogenized\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "\n",
    "# 3) Valid if:\n",
    "#    - tie_result == 1\n",
    "#    - OR tie_result == 0 and z_flag_homogenized == 6\n",
    "valid_mask = (tie_as_int.eq(1)) | (tie_as_int.eq(0) & zflag_as_int.eq(6))\n",
    "violations = na_cmp[~valid_mask].copy()\n",
    "\n",
    "# 4) Summary\n",
    "total_na     = len(na_cmp)\n",
    "valid_count  = int(valid_mask.sum())\n",
    "invalid_count = len(violations)\n",
    "\n",
    "print(\"===== compared_to <NA> validation (tie_result rule with flag-6 exception) =====\")\n",
    "print(f\"Rows with compared_to <NA>: {total_na}\")\n",
    "print(f\"Valid:                      {valid_count}\")\n",
    "print(f\"INVALID:                    {invalid_count}\")\n",
    "\n",
    "# Quick distribution to sanity-check what's inside the NA-compared set\n",
    "print(\"\\nCrosstab of tie_result x z_flag_homogenized (parsed ints) for <NA> compared_to:\")\n",
    "print(pd.crosstab(tie_as_int, zflag_as_int, dropna=False))\n",
    "\n",
    "# 5) Show some offending rows (if any)\n",
    "if invalid_count > 0:\n",
    "    cols_to_show = [\n",
    "        \"CRD_ID\", \"ra\", \"dec\", \"z\", \"z_flag\", \"z_err\",\n",
    "        \"z_flag_homogenized\", \"instrument_type\", \"instrument_type_homogenized\",\n",
    "        \"tie_result\", \"survey\", \"source\", \"compared_to\"\n",
    "    ]\n",
    "    print(\"\\n‚ö†Ô∏è Examples of violations (up to 10):\")\n",
    "    display(violations[cols_to_show].head(10))\n",
    "else:\n",
    "    print(\"\\n‚úÖ All rows with compared_to <NA> satisfy the rule (1, or 0 with flag==6).\")\n",
    "\n",
    "# 6) Optional hard assertion\n",
    "# assert invalid_count == 0, \"Found rows where compared_to is <NA> but tie_result is invalid (not 1, nor 0 with flag==6).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5b678-2cb8-455f-8dc9-4970dcd09924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Normalize compared_to so blanks/strings like \"nan\" become NA ---\n",
    "df_chk = df_final.copy()\n",
    "df_chk[\"compared_to_norm\"] = (\n",
    "    df_chk[\"compared_to\"]\n",
    "      .astype(\"string\").str.strip()\n",
    "      .replace({\"\": pd.NA, \"nan\": pd.NA, \"NaN\": pd.NA, \"NA\": pd.NA,\n",
    "                \"<NA>\": pd.NA, \"None\": pd.NA, \"null\": pd.NA})\n",
    ")\n",
    "\n",
    "# --- Check condition only on rows with z_flag_homogenized == 6 ---\n",
    "mask6 = df_chk[\"z_flag_homogenized\"].eq(6)\n",
    "all_na = df_chk.loc[mask6, \"compared_to_norm\"].isna().all()\n",
    "\n",
    "print(f\"Todos os z_flag_homogenized==6 t√™m compared_to <NA>? {all_na}\")\n",
    "\n",
    "# Optional: list offending rows if any\n",
    "if not all_na:\n",
    "    offenders = df_chk.loc[\n",
    "        mask6 & df_chk[\"compared_to_norm\"].notna(),\n",
    "        [\"CRD_ID\", \"z\", \"z_flag_homogenized\", \"survey\", \"source\", \"compared_to\", \"tie_result\"]\n",
    "    ].head(20)\n",
    "    display(offenders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801271a1-eeb4-402d-a97c-46de5cb47376",
   "metadata": {},
   "source": [
    "## Validation - Prepared Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c38638-040b-43cc-9c0b-fb22ba0e0dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dicion√°rio de regras equivalente ao YAML\n",
    "translation_rules = {\n",
    "    \"2DFGRS\": {\n",
    "        \"z_flag_translation\": {1: 0, 2: 1, 3: 3, 4: 4, 5: 4},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"2DFLENS\": {\n",
    "        \"z_flag_translation\": {1: 0, 2: 1, 3: 3, 4: 4, 6: 6},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"2MRS\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"z_err == 0\", \"value\": 3},\n",
    "                {\"expr\": \"0 < z_err < 0.0005\", \"value\": 4},\n",
    "                {\"expr\": \"z_err >= 0.0005\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"3D-HST\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"z_best_s == 0\", \"value\": 6},\n",
    "                {\"expr\": \"z_best_s == 1 and z_spec != -1\", \"value\": 4},\n",
    "                {\"expr\": \"z_best_s == 2 and use_zgrism == 1 and flag1 == 0 and flag2 == 0\", \"value\": 3},\n",
    "                {\"expr\": \"z_best_s == 3 and use_phot == 1\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"z_best_s == 1\", \"value\": \"s\"},\n",
    "                {\"expr\": \"z_best_s == 2\", \"value\": \"g\"},\n",
    "                {\"expr\": \"z_best_s == 3\", \"value\": \"p\"},\n",
    "            ],\n",
    "            \"default\": \"g\",\n",
    "        },\n",
    "    },\n",
    "    \"6DFGS\": {\n",
    "        \"z_flag_translation\": {1: 0, 2: 1, 3: 3, 4: 4, 6: 6},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"ASTRODEEP\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"zspec_survey != '-'\", \"value\": 4},\n",
    "                {\"expr\": \"zspec_survey == '-'\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"zspec_survey != '-'\", \"value\": \"s\"},\n",
    "                {\"expr\": \"zspec_survey == '-'\", \"value\": \"p\"},\n",
    "            ],\n",
    "            \"default\": \"p\",\n",
    "        },\n",
    "    },\n",
    "    \"ASTRODEEP-JWST\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"zspec != -99 and z_flag < 400 and (len(str(int(z_flag))) <= 1 or int(str(int(z_flag))[-2]) <= 3)\", \"value\": 4},\n",
    "                {\"expr\": \"zspec == -99 and z_flag < 400 and (len(str(int(z_flag))) <= 1 or int(str(int(z_flag))[-2]) <= 3)\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"zspec != -99\", \"value\": \"s\"},\n",
    "                {\"expr\": \"zspec == -99\", \"value\": \"p\"},\n",
    "            ],\n",
    "            \"default\": \"p\",\n",
    "        },\n",
    "    },\n",
    "    \"DESI\": {\n",
    "        \"z_flag_translation\": {\n",
    "            \"conditions\": [\n",
    "                {\"expr\": \"ZCAT_PRIMARY != True\", \"value\": 0},\n",
    "                {\"expr\": \"z_flag != 0 and ZCAT_PRIMARY == True\", \"value\": 1},\n",
    "                {\"expr\": \"z_flag == 0 and ZCAT_PRIMARY == True and z_err < 0.0005\", \"value\": 4},\n",
    "                {\"expr\": \"z_flag == 0 and ZCAT_PRIMARY == True and z_err >= 0.0005\", \"value\": 3},\n",
    "            ],\n",
    "            \"default\": 0,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"JADES\": {\n",
    "        \"z_flag_translation\": {4: 4, 3: 3, 2: 2, 1: 1, 0: 0},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"MOSDEF\": {\n",
    "        \"z_flag_translation\": {7: 4, 6: 3, 5: 2, 4: 2, 3: 1, 2: 1, 1: 0, 0: 0},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"OZDES\": {\n",
    "        \"z_flag_translation\": {1: 0, 2: 1, 3: 3, 4: 4, 6: 6},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"PRIMUS\": {\n",
    "        \"z_flag_translation\": {-1: 0, 2: 1, 3: 2, 4: 3},\n",
    "        \"instrument_type_translation\": {\"default\": \"g\"},\n",
    "    },\n",
    "    \"VANDELS\": {\n",
    "        \"z_flag_translation\": {\n",
    "            0: 0, 1: 1, 2: 2, 3: 4, 4: 4, 9: 3,\n",
    "            10: 0, 11: 1, 12: 2, 13: 4, 14: 4, 19: 3,\n",
    "            20: 0, 21: 1, 22: 2, 23: 4, 24: 4, 29: 3,\n",
    "            210: 0, 211: 1, 212: 2, 213: 4, 214: 4, 219: 3,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"VIMOS\": {\n",
    "        \"z_flag_translation\": {4: 4, 3: 3, 2: 2, 1: 1, 0: 0},\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"VUDS\": {\n",
    "        \"z_flag_translation\": {\n",
    "            1: 1, 11: 1, 21: 1, 31: 1, 41: 1,\n",
    "            2: 2, 12: 2, 22: 2, 32: 2, 42: 2, 9: 2, 19: 2, 29: 2, 39: 2, 49: 2,\n",
    "            3: 3, 13: 3, 23: 3, 33: 3, 43: 3,\n",
    "            4: 4, 14: 4, 24: 4, 34: 4, 44: 4,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "    \"VVDS\": {\n",
    "        \"z_flag_translation\": {\n",
    "            0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 9: 2,\n",
    "            10: 0, 11: 1, 12: 2, 13: 3, 14: 4, 19: 2,\n",
    "            20: 0, 21: 1, 22: 2, 23: 3, 24: 4, 29: 2,\n",
    "            210: 0, 211: 1, 212: 2, 213: 3, 214: 4, 219: 2,\n",
    "        },\n",
    "        \"instrument_type_translation\": {\"default\": \"s\"},\n",
    "    },\n",
    "\n",
    "    # Special cases using continuous rule and inherited type\n",
    "    \"CANDELS\": {\"_special\": \"CANDELS_NED\"},\n",
    "    \"NED\": {\"_special\": \"CANDELS_NED\"},\n",
    "}\n",
    "\n",
    "def _safe_eval_expr(expr: str, ctx: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Avalia 'expr' usando apenas vari√°veis do ctx e fun√ß√µes b√°sicas.\n",
    "    Retorna True/False; se der erro, retorna False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Permitir apenas fun√ß√µes b√°sicas e numpy\n",
    "        allowed_globals = {\n",
    "            \"__builtins__\": {\"len\": len, \"int\": int, \"str\": str, \"float\": float},\n",
    "            \"np\": np,\n",
    "        }\n",
    "        return bool(eval(expr, allowed_globals, ctx))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _apply_translation(value_map, row_ctx):\n",
    "    \"\"\"\n",
    "    value_map pode ser:\n",
    "      - dict simples {orig: dest} (pode conter 'default')\n",
    "      - dict com 'conditions' (lista de {expr, value}) e opcional 'default'\n",
    "    Retorna (valor_traduzido, matched_bool)\n",
    "    \"\"\"\n",
    "    if isinstance(value_map, dict) and \"conditions\" in value_map:\n",
    "        for cond in value_map[\"conditions\"]:\n",
    "            expr = cond.get(\"expr\", \"\")\n",
    "            val = cond.get(\"value\", np.nan)\n",
    "            if expr and _safe_eval_expr(expr, row_ctx):\n",
    "                return val, True\n",
    "        # nenhum matched -> usa default se houver\n",
    "        if \"default\" in value_map:\n",
    "            return value_map[\"default\"], True\n",
    "        return np.nan, False\n",
    "\n",
    "    # mapeamento direto (sem 'conditions'):\n",
    "    if isinstance(value_map, dict):\n",
    "        key = row_ctx.get(\"z_flag\", np.nan)\n",
    "        if key in value_map:\n",
    "            return value_map[key], True\n",
    "        # Se n√£o houver chave correspondente, mas existir 'default', use-o\n",
    "        if \"default\" in value_map:\n",
    "            return value_map[\"default\"], True\n",
    "        return np.nan, False\n",
    "\n",
    "    return np.nan, False\n",
    "\n",
    "def validate_row(row):\n",
    "    survey = row.get(\"survey\", None)\n",
    "\n",
    "    # construir contexto com None -> np.nan, para evitar erros de compara√ß√£o\n",
    "    ctx = {}\n",
    "    for k, v in row.items():\n",
    "        ctx[k] = (np.nan if v is None else v)\n",
    "\n",
    "    # Casos especiais (CANDELS e NED): regra cont√≠nua 0..1 e type herdado\n",
    "    if survey in (\"CANDELS\", \"NED\"):\n",
    "        x = row.get(\"z_flag\", np.nan)\n",
    "        # z_flag esperado:\n",
    "        if x == 0.0:\n",
    "            z_expected = 0.0\n",
    "        elif (isinstance(x, (float, int))) and (0.0 < x < 0.7):\n",
    "            z_expected = 1.0\n",
    "        elif (isinstance(x, (float, int))) and (0.7 <= x < 0.9):\n",
    "            z_expected = 2.0\n",
    "        elif (isinstance(x, (float, int))) and (0.9 <= x < 0.99):\n",
    "            z_expected = 3.0\n",
    "        elif (isinstance(x, (float, int))) and (0.99 <= x <= 1.0):\n",
    "            z_expected = 4.0\n",
    "        else:\n",
    "            z_expected = np.nan\n",
    "\n",
    "        # type_expected √© o pr√≥prio 'type' da linha\n",
    "        type_expected = row.get(\"instrument_type\", np.nan)\n",
    "        return z_expected, type_expected\n",
    "\n",
    "    # Regras gerais dos surveys\n",
    "    rules = translation_rules.get(survey, None)\n",
    "    if rules is None:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    # z_flag_homogenized esperado\n",
    "    z_rules = rules.get(\"z_flag_translation\", None)\n",
    "    if z_rules is None:\n",
    "        z_expected = np.nan\n",
    "    else:\n",
    "        z_expected, _ = _apply_translation(z_rules, ctx)\n",
    "\n",
    "    # instrument_type_homogenized esperado\n",
    "    t_rules = rules.get(\"instrument_type_translation\", None)\n",
    "    if t_rules is None:\n",
    "        type_expected = np.nan\n",
    "    else:\n",
    "        if isinstance(t_rules, dict) and (\"conditions\" in t_rules or \"default\" in t_rules):\n",
    "            type_expected, matched = _apply_translation(t_rules, ctx)\n",
    "        else:\n",
    "            type_expected, matched = _apply_translation(t_rules, ctx)\n",
    "\n",
    "\n",
    "    return z_expected, type_expected\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# VALIDATE TRANSLATIONS IN TEMP FILES\n",
    "# =========================================================\n",
    "merged_files = glob.glob(os.path.join(prepared_temp_dir, \"prepared*/*.parquet\"))\n",
    "merged_files = [f for f in merged_files if \"pipeline_sample\" not in f]\n",
    "\n",
    "if not merged_files:\n",
    "    print(\"‚ö†Ô∏è No prepared parquet files found for validation.\")\n",
    "else:\n",
    "    issues = []\n",
    "    \n",
    "    for merged_file in merged_files:\n",
    "        print(f\"üîç Validating {merged_file}\")\n",
    "        df = pd.read_parquet(merged_file)\n",
    "    \n",
    "        for _, row in df.iterrows():\n",
    "            z_exp, type_exp = validate_row(row)\n",
    "    \n",
    "            if not (pd.isna(z_exp) and pd.isna(row[\"z_flag_homogenized\"])) and z_exp != row[\"z_flag_homogenized\"]:\n",
    "                issue = row.to_dict()\n",
    "                issue[\"field\"] = \"z_flag_homogenized\"\n",
    "                issue[\"expected\"] = z_exp\n",
    "                issue[\"found\"] = row[\"z_flag_homogenized\"]\n",
    "                issues.append(issue)\n",
    "    \n",
    "            if not (pd.isna(type_exp) and pd.isna(row[\"instrument_type_homogenized\"])) and type_exp != row[\"instrument_type_homogenized\"]:\n",
    "                issue = row.to_dict()\n",
    "                issue[\"field\"] = \"instrument_type_homogenized\"\n",
    "                issue[\"expected\"] = type_exp\n",
    "                issue[\"found\"] = row[\"instrument_type_homogenized\"]\n",
    "                issues.append(issue)\n",
    "    \n",
    "    if issues:\n",
    "        issues_df = pd.DataFrame(issues)\n",
    "        display(issues_df)\n",
    "        print(f\"‚ö†Ô∏è {len(issues)} mismatches found!\")\n",
    "    else:\n",
    "        print(\"‚úÖ All homogenized fields match the expected values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede0adcb-e34d-4e81-90e1-8079773eedb6",
   "metadata": {},
   "source": [
    "# Time Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417dc417-cee6-45ba-8306-952ac9bf03f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================================\n",
    "# 1. CONFIGURA√á√ÉO\n",
    "# ============================================\n",
    "\n",
    "log_dir = \"process001/process_info\"\n",
    "\n",
    "log_files = [\n",
    "    \"prepare_all.log\",\n",
    "    \"import_all.log\",\n",
    "    \"margin_cache_all.log\",\n",
    "    \"crossmatch_and_merge_all.log\",\n",
    "    \"process.log\"\n",
    "]\n",
    "\n",
    "START_RE = re.compile(\n",
    "    r\"(?P<timestamp>\\d{4}-\\d{2}-\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d+): Starting: (?P<task>[\\w_]+) id=(?P<id>[\\w\\d_]+)\"\n",
    ")\n",
    "FINISH_RE = re.compile(\n",
    "    r\"(?P<timestamp>\\d{4}-\\d{2}-\\d{2}-\\d{2}:\\d{2}:\\d{2}\\.\\d+): Finished: (?P<task>[\\w_]+) id=(?P<id>[\\w\\d_]+)\"\n",
    ")\n",
    "\n",
    "start_times = {}\n",
    "end_times = {}\n",
    "\n",
    "# ============================================\n",
    "# 2. LEITURA E PARSE DOS LOGS\n",
    "# ============================================\n",
    "\n",
    "for file in log_files:\n",
    "    path = os.path.join(log_dir, file)\n",
    "    if not os.path.exists(path):\n",
    "        continue\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            m_start = START_RE.search(line)\n",
    "            m_finish = FINISH_RE.search(line)\n",
    "\n",
    "            if m_start:\n",
    "                task_id = f\"{m_start.group('task')}|{m_start.group('id')}\"\n",
    "                if task_id not in start_times:\n",
    "                    start_times[task_id] = datetime.strptime(\n",
    "                        m_start.group(\"timestamp\"), \"%Y-%m-%d-%H:%M:%S.%f\"\n",
    "                    )\n",
    "\n",
    "            if m_finish:\n",
    "                task_id = f\"{m_finish.group('task')}|{m_finish.group('id')}\"\n",
    "                if task_id not in end_times:\n",
    "                    end_times[task_id] = datetime.strptime(\n",
    "                        m_finish.group(\"timestamp\"), \"%Y-%m-%d-%H:%M:%S.%f\"\n",
    "                    )\n",
    "\n",
    "# ============================================\n",
    "# 2b. AJUSTAR TEMPOS DOS PREPARE_CATALOGS\n",
    "# ============================================\n",
    "\n",
    "prepare_ids = [tid for tid in start_times if tid.startswith(\"prepare_catalog|\") and tid != \"prepare_catalogs|prepare_catalogs\"]\n",
    "prepare_catalogs_id = \"prepare_catalogs|prepare_catalogs\"\n",
    "\n",
    "if prepare_catalogs_id in start_times:\n",
    "    general_prepare_start = start_times[prepare_catalogs_id]\n",
    "    for tid in prepare_ids:\n",
    "        start_times[tid] = general_prepare_start\n",
    "\n",
    "# ============================================\n",
    "# 3. CONSTRU√á√ÉO DO EIXO Y EM ORDEM CUSTOMIZADA\n",
    "# ============================================\n",
    "\n",
    "all_ids = sorted(set(start_times) & set(end_times))\n",
    "\n",
    "pipeline_init_id = \"pipeline_init|pipeline_init\"\n",
    "consolidate_id = \"consolidate|consolidate\"\n",
    "\n",
    "import_cat0 = [tid for tid in all_ids if tid == \"import_catalog|cat0_hats\"]\n",
    "\n",
    "remaining_ids = [tid for tid in all_ids if tid not in prepare_ids + import_cat0 + [pipeline_init_id, consolidate_id]]\n",
    "\n",
    "step_dict = defaultdict(list)\n",
    "for tid in remaining_ids:\n",
    "    match = re.search(r\"(?:cat|merged_step)(\\d+)\", tid)\n",
    "    if match:\n",
    "        step = int(match.group(1))\n",
    "        step_dict[step].append(tid)\n",
    "\n",
    "ordered_step_ids = []\n",
    "for step in sorted(step_dict):\n",
    "    step_tasks = step_dict[step]\n",
    "\n",
    "    def task_order(tid):\n",
    "        if tid.startswith(\"import_catalog|cat\"):\n",
    "            return 0\n",
    "        elif tid.startswith(\"generate_margin_cache\"):\n",
    "            return 1\n",
    "        elif tid.startswith(\"crossmatch_and_merge\"):\n",
    "            return 2\n",
    "        elif tid.startswith(\"import_catalog|merged_step\"):\n",
    "            return 3\n",
    "        else:\n",
    "            return 99\n",
    "\n",
    "    ordered_step_ids.extend(sorted(step_tasks, key=task_order))\n",
    "\n",
    "ordered_ids = [pipeline_init_id] + prepare_ids + import_cat0 + ordered_step_ids + [consolidate_id]\n",
    "\n",
    "# ============================================\n",
    "# 4. MONTAGEM DOS DADOS PARA O PLOT\n",
    "# ============================================\n",
    "\n",
    "# Tempo adicional a ser subtra√≠do do in√≠cio do pipeline_init (em segundos)\n",
    "aditional_pipeline_init_time = 3  # ‚è±Ô∏è ajuste aqui conforme necess√°rio\n",
    "\n",
    "# üõ†Ô∏è Aplicar tempo extra retroativo ao in√≠cio do pipeline_init\n",
    "start_times[pipeline_init_id] -= timedelta(seconds=aditional_pipeline_init_time)\n",
    "\n",
    "# Novo zero do gr√°fico com base nesse novo tempo\n",
    "start_zero = min(start_times[tid] for tid in ordered_ids)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# üõ†Ô∏è INSERIR REGISTRO MANUAL DA TAREFA \"register\"\n",
    "# --------------------------------------------------\n",
    "register_id = \"register|register\"\n",
    "register_duration = 3  # ‚è±Ô∏è ajuste aqui a dura√ß√£o da tarefa \"register\" em segundos\n",
    "\n",
    "ordered_ids.append(register_id)\n",
    "register_start = max(end_times[consolidate_id], *end_times.values())\n",
    "register_end = register_start + timedelta(seconds=register_duration)\n",
    "\n",
    "start_times[register_id] = register_start\n",
    "end_times[register_id] = register_end\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Recalcular dados do gr√°fico com tempos relativos ao novo start_zero\n",
    "y_labels = []\n",
    "start_list = []\n",
    "end_list = []\n",
    "\n",
    "for tid in ordered_ids:\n",
    "    y_labels.append(tid)\n",
    "    start_rel = (start_times[tid] - start_zero).total_seconds()\n",
    "    end_rel = (end_times[tid] - start_zero).total_seconds()\n",
    "    start_list.append(start_rel)\n",
    "    end_list.append(end_rel)\n",
    "\n",
    "# ============================================\n",
    "# 4b. AJUSTAR POSI√á√ïES Y PARA SEPARAR \"register\"\n",
    "# ============================================\n",
    "\n",
    "# Cria posi√ß√µes Y padr√£o e separa o √∫ltimo (register) com um espa√ßamento extra\n",
    "y_positions = list(range(len(ordered_ids)))\n",
    "y_positions[-1] += 5.0  # üõ†Ô∏è Aumenta a posi√ß√£o do \"register\" no eixo Y\n",
    "\n",
    "# ============================================\n",
    "# 5. PLOTAGEM DO GR√ÅFICO DE TIME PROFILE\n",
    "# ============================================\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# === Mapear cores por grupo\n",
    "group_colors = {\n",
    "    \"pipeline_init\": \"#003f5c\",       # azul escuro\n",
    "    \"prepare_catalogs\": \"#b8860b\",    # amarelo escuro\n",
    "    \"crossmatch\": \"#2f855a\",          # verde escuro\n",
    "    \"consolidate\": \"#003f5c\",         # bege escuro\n",
    "    \"register\": \"#003f5c\",            # üõ†Ô∏è mesmo azul escuro do pipeline_init\n",
    "}\n",
    "\n",
    "# === Determinar grupo de cada tarefa\n",
    "def get_group(tid):\n",
    "    if tid == pipeline_init_id:\n",
    "        return \"pipeline_init\"\n",
    "    elif tid == register_id:\n",
    "        return \"register\"\n",
    "    elif tid in prepare_ids:\n",
    "        return \"prepare_catalogs\"\n",
    "    elif tid == consolidate_id:\n",
    "        return \"consolidate\"\n",
    "    else:\n",
    "        return \"crossmatch\"\n",
    "\n",
    "# === Plotar tarefas com cor unificada para linha e bolinhas\n",
    "for y, start, end, tid in zip(y_positions, start_list, end_list, ordered_ids):\n",
    "    group = get_group(tid)\n",
    "    color = group_colors[group]\n",
    "\n",
    "    plt.hlines(y, start, end, colors=color, linewidth=2)\n",
    "    plt.scatter(start, y, color=color, s=10)  # üü¢ mesmo tom da linha (in√≠cio)\n",
    "    plt.scatter(end, y, color=color, s=10)    # üî¥ mesmo tom da linha (fim)\n",
    "\n",
    "# ============================================\n",
    "# Agrupar labels do eixo Y por grupo\n",
    "# ============================================\n",
    "\n",
    "group_positions = defaultdict(list)\n",
    "for y, tid in zip(y_positions, ordered_ids):\n",
    "    group_positions[get_group(tid)].append(y)\n",
    "\n",
    "group_labels = []\n",
    "group_ticks = []\n",
    "\n",
    "for label in [\"pipeline_init\", \"prepare_catalogs\", \"crossmatch\", \"consolidate\", \"register\"]:\n",
    "    if group_positions[label]:\n",
    "        center = sum(group_positions[label]) / len(group_positions[label])\n",
    "        group_labels.append(label)\n",
    "        group_ticks.append(center)\n",
    "\n",
    "# ============================================\n",
    "# Personaliza√ß√£o final do gr√°fico\n",
    "# ============================================\n",
    "\n",
    "plt.yticks(group_ticks, group_labels, fontsize=20)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.xlabel(\"Time (s)\", fontsize=20)\n",
    "#plt.ylabel(\"Task Group\", fontsize=25)\n",
    "#plt.title(\"Time Profile\", fontsize=30)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_preparation",
   "language": "python",
   "name": "data_preparation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
