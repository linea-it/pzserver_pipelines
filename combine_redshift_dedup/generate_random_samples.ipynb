{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99edb3dc-03b4-4378-8496-d910211ff804",
   "metadata": {},
   "source": [
    "# Generating random samples for many files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae52ee47-d0de-4033-95ab-465c7517af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# ----------------- CONFIG -----------------\n",
    "# Directories containing your input Parquet files\n",
    "INPUT_DIRS = [\n",
    "    \"/scratch/users/luigi.silva/speczs-catalogs/processed\",\n",
    "    \"/scratch/users/luigi.silva/speczs-catalogs/johns-catalogs\",\n",
    "]\n",
    "\n",
    "# Directory where the sampled files will be saved\n",
    "OUTPUT_DIR = Path(\"/scratch/users/luigi.silva/pzserver_pipelines/combine_redshift_dedup/test_data\")\n",
    "\n",
    "# Maximum number of rows per sample\n",
    "SAMPLE_MAX = 1000\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42\n",
    "# ------------------------------------------\n",
    "\n",
    "\n",
    "def ensure_outdir(path: Path) -> None:\n",
    "    \"\"\"Make sure the output directory exists.\"\"\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def sample_parquet_reservoir(in_path: Path, k: int, seed: int = 42) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Perform random sampling (without replacement) from a Parquet file.\n",
    "    - If the file has <= k rows, return all rows.\n",
    "    - Otherwise, use reservoir sampling to select k rows while reading in streaming mode.\n",
    "      This avoids loading the entire file into memory.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    pf = pq.ParquetFile(str(in_path))\n",
    "    total_rows = pf.metadata.num_rows\n",
    "\n",
    "    # Case 1: small file -> just read all rows\n",
    "    if total_rows <= k:\n",
    "        return pf.read()\n",
    "\n",
    "    # Case 2: large file -> reservoir sampling\n",
    "    schema = pf.schema_arrow\n",
    "    columns = [[] for _ in schema]  # temporary storage for sampled rows\n",
    "\n",
    "    seen = 0  # number of rows processed so far\n",
    "\n",
    "    for batch in pf.iter_batches():\n",
    "        # Get columns as arrays for easier row access\n",
    "        cols = [batch.column(i) for i in range(batch.num_columns)]\n",
    "        n = batch.num_rows\n",
    "\n",
    "        for i in range(n):\n",
    "            if seen < k:\n",
    "                # Fill the reservoir until it reaches size k\n",
    "                for c_idx, arr in enumerate(cols):\n",
    "                    columns[c_idx].append(arr[i].as_py())\n",
    "            else:\n",
    "                # Replace elements with decreasing probability\n",
    "                j = random.randint(0, seen)\n",
    "                if j < k:\n",
    "                    for c_idx, arr in enumerate(cols):\n",
    "                        columns[c_idx][j] = arr[i].as_py()\n",
    "            seen += 1\n",
    "\n",
    "    # Convert sampled Python lists back to Arrow arrays\n",
    "    arrays = [pa.array(col, type=schema.field(i).type) for i, col in enumerate(columns)]\n",
    "    table = pa.Table.from_arrays(arrays, names=[f.name for f in schema])\n",
    "\n",
    "    assert len(table) == min(k, total_rows)\n",
    "    return table\n",
    "\n",
    "\n",
    "# ----------------- MAIN EXECUTION -----------------\n",
    "ensure_outdir(OUTPUT_DIR)\n",
    "count = 0\n",
    "\n",
    "for d in INPUT_DIRS:\n",
    "    in_dir = Path(d)\n",
    "    if not in_dir.is_dir():\n",
    "        print(f\"[WARN] Directory not found: {in_dir}\")\n",
    "        continue\n",
    "\n",
    "    for p in sorted(in_dir.glob(\"*.parquet\")):\n",
    "        survey_name = p.stem  # file name without extension\n",
    "        out_path = OUTPUT_DIR / f\"{survey_name}_random_sample.parquet\"\n",
    "\n",
    "        print(f\"[INFO] Sampling {p} -> {out_path}\")\n",
    "        tbl = sample_parquet_reservoir(p, SAMPLE_MAX, seed=SEED)\n",
    "\n",
    "        pq.write_table(tbl, out_path)\n",
    "        count += 1\n",
    "\n",
    "print(f\"[DONE] {count} files processed. Output in: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e893d-2203-4741-990c-215b0a2e6f81",
   "metadata": {},
   "source": [
    "# Generating random sample for a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf3a2d-7878-4fb5-8ec8-328e89022bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# --------------- CONFIG ---------------\n",
    "# Single input Parquet file (extension \"parqut\" is fine as long as the file exists and is Parquet)\n",
    "INPUT_FILE = Path(\"/scratch/users/luigi.silva/pzserver_pipelines/combine_redshift_dedup/process001/crd.parquet\")\n",
    "\n",
    "# Exact output path\n",
    "OUTPUT_PATH = Path(\"./test_data/pipeline_generated_sample.parquet\")\n",
    "\n",
    "# Maximum number of rows in the sample\n",
    "SAMPLE_MAX = 1000\n",
    "\n",
    "# Random seed for reproducibility\n",
    "SEED = 42\n",
    "# --------------------------------------\n",
    "\n",
    "\n",
    "def sample_parquet_reservoir(in_path: Path, k: int, seed: int = 42) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Return a random sample (without replacement) of up to k rows from a Parquet file.\n",
    "    - If the file has <= k rows, return the whole file.\n",
    "    - Otherwise, use reservoir sampling while streaming through the file in row batches\n",
    "      (avoids loading the entire file into memory).\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    pf = pq.ParquetFile(str(in_path))\n",
    "    total_rows = pf.metadata.num_rows\n",
    "    schema = pf.schema_arrow\n",
    "\n",
    "    # Handle empty files explicitly\n",
    "    if total_rows == 0:\n",
    "        arrays = [pa.array([], type=schema.field(i).type) for i in range(len(schema))]\n",
    "        return pa.Table.from_arrays(arrays, names=[f.name for f in schema])\n",
    "\n",
    "    # Small file -> just read all rows\n",
    "    if total_rows <= k:\n",
    "        return pf.read()\n",
    "\n",
    "    # Large file -> reservoir sampling\n",
    "    columns = [[] for _ in range(len(schema))]  # temporary storage for sampled rows\n",
    "    seen = 0  # number of rows processed so far\n",
    "\n",
    "    for batch in pf.iter_batches():\n",
    "        # Access columns as Arrow arrays for quick row access\n",
    "        cols = [batch.column(i) for i in range(batch.num_columns)]\n",
    "        n = batch.num_rows\n",
    "\n",
    "        for i in range(n):\n",
    "            if seen < k:\n",
    "                # Fill the reservoir until it reaches size k\n",
    "                for c_idx, arr in enumerate(cols):\n",
    "                    columns[c_idx].append(arr[i].as_py())\n",
    "            else:\n",
    "                # Replace elements with decreasing probability\n",
    "                j = random.randint(0, seen)\n",
    "                if j < k:\n",
    "                    for c_idx, arr in enumerate(cols):\n",
    "                        columns[c_idx][j] = arr[i].as_py()\n",
    "            seen += 1\n",
    "\n",
    "    # Convert sampled Python lists back to Arrow arrays with the original schema types\n",
    "    arrays = [pa.array(col, type=schema.field(i).type) for i, col in enumerate(columns)]\n",
    "    table = pa.Table.from_arrays(arrays, names=[f.name for f in schema])\n",
    "\n",
    "    assert len(table) == min(k, total_rows), \"Sample size mismatch\"\n",
    "    return table\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not INPUT_FILE.is_file():\n",
    "        raise FileNotFoundError(f\"Input Parquet not found: {INPUT_FILE}\")\n",
    "\n",
    "    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"[INFO] Sampling {INPUT_FILE} -> {OUTPUT_PATH}\")\n",
    "    tbl = sample_parquet_reservoir(INPUT_FILE, SAMPLE_MAX, seed=SEED)\n",
    "    pq.write_table(tbl, OUTPUT_PATH)\n",
    "    print(f\"[DONE] {len(tbl)} rows written to: {OUTPUT_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_preparation",
   "language": "python",
   "name": "data_preparation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
